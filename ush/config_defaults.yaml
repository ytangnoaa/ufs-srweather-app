#----------------------------
# Test description
#----------------------------
metadata:
  description: >-
    Default configuration for an experiment. The valid values for most of the
    parameters are specified in valid_param_vals.yaml
  version: !!str '1.0'
#----------------------------
# USER config parameters
#----------------------------
user:
  #
  #-----------------------------------------------------------------------
  #
  # Set the RUN_ENVIR variable that is listed and described in the WCOSS
  # Implementation Standards document:
  #
  #   NCEP Central Operations
  #   WCOSS Implementation Standards
  #   April 19, 2022
  #   Version 11.0.0
  #
  # RUN_ENVIR is described in this document as follows:
  #
  #   Set to "nco" if running in NCO's production environment. Used to 
  #   distinguish between organizations.
  #
  # Valid values are "nco" and "community".  Here, we use it to generate
  # and run the experiment either in NCO mode (if RUN_ENVIR is set to "nco")
  # or in community mode (if RUN_ENVIR is set to "community").  This has 
  # implications on the experiment variables that need to be set and the
  # the directory structure used.
  #
  #-----------------------------------------------------------------------
  #
  RUN_ENVIR: "nco"
  #
  #-----------------------------------------------------------------------
  #
  # Set machine and queue parameters.  Definitions:
  #
  # MACHINE:
  # Machine on which the workflow will run. If you are NOT on a named,
  # supported platform, and you want to use the Rocoto workflow manager,
  # you will need set MACHINE: "linux" and WORKFLOW_MANAGER: "rocoto". This
  # combination will assume a Slurm batch manager when generating the XML.
  # Please see ush/valid_param_vals.yaml for a full list of supported
  # platforms.
  #
  # ACCOUNT:
  # The account under which to submit jobs to the queue.
  #
  #-----------------------------------------------------------------------
  MACHINE: "BIG_COMPUTER"
  ACCOUNT: ""

  HOMEaqm: '{{ user.HOMEaqm }}'
  USHdir: '{{ user.USHdir }}'
  SCRIPTSdir: '{{ [HOMEaqm, "scripts"]|path_join }}'
  JOBSdir: '{{ [HOMEaqm, "jobs"]|path_join }}'
  SORCdir: '{{ [HOMEaqm, "sorc"]|path_join }}'
  PARMdir: '{{ [HOMEaqm, "parm"]|path_join }}'
  MODULESdir: '{{ [HOMEaqm, "modulefiles"]|path_join }}'
  EXECdir: '{{ [HOMEaqm, workflow.EXEC_SUBDIR]|path_join }}'
  VX_CONFIG_DIR: '{{ [HOMEaqm, "parm"]|path_join }}'
  METPLUS_CONF: '{{ [PARMdir, "metplus"]|path_join }}'
  MET_CONFIG: '{{ [PARMdir, "met"]|path_join }}'
  UFS_WTHR_MDL_DIR: '{{ user.UFS_WTHR_MDL_DIR }}'
  ARL_NEXUS_DIR: '{{ [SORCdir, "arl_nexus"]|path_join }}'

#----------------------------
# PLATFORM config parameters
#-----------------------------
platform:
  #
  #-----------------------------------------------------------------------
  #
  # WORKFLOW_MANAGER:
  # The workflow manager to use (e.g. rocoto). This is set to "none" by
  # default, but if the machine name is set to a platform that supports
  # rocoto, this will be overwritten and set to "rocoto". If set
  # explicitly to rocoto along with the use of the MACHINE=linux target,
  # the configuration layer assumes a Slurm batch manager when generating
  # the XML. Valid options: "rocoto" or "none"
  #
  # NCORES_PER_NODE:
  # The number of cores available per node on the compute platform, now 
  # configurable for all platforms.
  #
  # TASKTHROTTLE:
  # The number of active tasks run simultaneously. For linux/mac setting this
  # to 1 makes sense
  #
  # BUILD_MOD_FN:
  # Name of alternative build module file to use if using an
  # unsupported platform. Is set automatically for supported machines.
  #
  # WFLOW_MOD_FN:
  # Name of alternative workflow module file to use if using an
  # unsupported platform. Is set automatically for supported machines.
  #
  # BUILD_VER_FN:
  # File name containing the version of the modules used for building the app.
  # Currently, WCOSS2 only uses this file.
  #
  # RUN_VER_FN:
  # File name containing the version of the modules used for running the app.
  # Currently, WCOSS2 only uses this file.
  #
  # SCHED:
  # The job scheduler to use (e.g. slurm).  Set this to an empty string in
  # order for the experiment generation script to set it depending on the
  # machine.
  #
  # PARTITION_DEFAULT:
  # If using the slurm job scheduler (i.e. if SCHED is set to "slurm"), 
  # the default partition to which to submit workflow tasks.  If a task 
  # does not have a specific variable that specifies the partition to which 
  # it will be submitted (e.g. PARTITION_HPSS, PARTITION_FCST; see below), 
  # it will be submitted to the partition specified by this variable.  If 
  # this is not set or is set to an empty string, it will be (re)set to a 
  # machine-dependent value.  This is not used if SCHED is not set to 
  # "slurm".
  #
  # QUEUE_DEFAULT:
  # The default queue or QOS (if using the slurm job scheduler, where QOS
  # is Quality of Service) to which workflow tasks are submitted.  If a 
  # task does not have a specific variable that specifies the queue to which 
  # it will be submitted (e.g. QUEUE_HPSS, QUEUE_FCST; see below), it will 
  # be submitted to the queue specified by this variable.  If this is not 
  # set or is set to an empty string, it will be (re)set to a machine-
  # dependent value.
  #
  # PARTITION_HPSS:
  # If using the slurm job scheduler (i.e. if SCHED is set to "slurm"), 
  # the partition to which the tasks that get or create links to external 
  # model files [which are needed to generate initial conditions (ICs) and 
  # lateral boundary conditions (LBCs)] are submitted.  If this is not set 
  # or is set to an empty string, it will be (re)set to a machine-dependent 
  # value.  This is not used if SCHED is not set to "slurm".
  #
  # QUEUE_HPSS:
  # The queue or QOS to which the tasks that get or create links to external 
  # model files [which are needed to generate initial conditions (ICs) and 
  # lateral boundary conditions (LBCs)] are submitted.  If this is not set 
  # or is set to an empty string, it will be (re)set to a machine-dependent 
  # value.
  #
  # PARTITION_FCST:
  # If using the slurm job scheduler (i.e. if SCHED is set to "slurm"), 
  # the partition to which the task that runs forecasts is submitted.  If 
  # this is not set or set to an empty string, it will be (re)set to a 
  # machine-dependent value.  This is not used if SCHED is not set to 
  # "slurm".
  #
  # QUEUE_FCST:
  # The queue or QOS to which the task that runs a forecast is submitted.  
  # If this is not set or set to an empty string, it will be (re)set to a 
  # machine-dependent value.
  #
  #-----------------------------------------------------------------------
  #
  WORKFLOW_MANAGER: ""
  NCORES_PER_NODE: ""
  TASKTHROTTLE: 1000
  BUILD_MOD_FN: 'build_{{ user.MACHINE|lower() }}_{{ workflow.COMPILER }}'
  WFLOW_MOD_FN: 'wflow_{{ user.MACHINE|lower() }}'
  BUILD_VER_FN: 'build.ver.{{ user.MACHINE|lower() }}'
  RUN_VER_FN: 'run.ver.{{ user.MACHINE|lower() }}'
  SCHED: ""
  PARTITION_DEFAULT: ""
  QUEUE_DEFAULT: ""
  PARTITION_HPSS: ""
  QUEUE_HPSS: ""
  PARTITION_FCST: ""
  QUEUE_FCST: ""
  #
  #-----------------------------------------------------------------------
  #
  # Set run commands for platforms without a workflow manager. These values
  # will be ignored unless WORKFLOW_MANAGER: "none".  Definitions:
  #
  # RUN_CMD_UTILS:
  # The run command for pre-processing utilities (shave, orog, sfc_climo_gen, 
  # etc.) Can be left blank for smaller domains, in which case the executables 
  # will run without MPI.
  #
  # RUN_CMD_FCST:
  # The run command for the model forecast step. 
  #
  # RUN_CMD_POST:
  # The run command for post-processing (UPP). Can be left blank for smaller 
  # domains, in which case UPP will run without MPI.
  #
  # RUN_CMD_PRDGEN: 
  # The run command for the product generation job.
  #
  # RUN_CMD_SERIAL:
  # The run command for some serial jobs 
  #
  # RUN_CMD_AQM:
  # The run command for some AQM tasks.
  #
  # RUN_CMD_AQMLBC:
  # The run command for the AQM_LBCS task.
  #
  #-----------------------------------------------------------------------
  #
  RUN_CMD_SERIAL: ""
  RUN_CMD_UTILS: ""
  RUN_CMD_FCST: ""
  RUN_CMD_POST: ""
  RUN_CMD_PRDGEN: ""
  RUN_CMD_AQM: ""
  RUN_CMD_AQMLBC: ""

  #
  #-----------------------------------------------------------------------
  #
  # Allows an extra parameter to be passed to SCHEDULER (SLURM/PBSPRO) via 
  # XML Native command
  #
  SCHED_NATIVE_CMD: ""

  #
  #-----------------------------------------------------------------------
  #
  # Set METplus parameters.  Definitions:
  #
  # MET_INSTALL_DIR:
  # Location to top-level directory of MET installation.
  #
  # MET_BIN_EXEC:
  # Subdirectory containing MET binaries e.g. "bin"
  #
  # METPLUS_PATH:
  # Location to top-level directory of METplus installation.
  #
  # MET_BIN_EXEC
  # Name of subdirectory where METplus executables are installed.
  #
  # CCPA_OBS_DIR:
  # User-specified location of top-level directory where CCPA hourly
  # precipitation files used by METplus are located. This parameter needs
  # to be set for both user-provided observations and for observations 
  # that are retrieved from the NOAA HPSS (if the user has access) via
  # the TN_GET_OBS_CCPA task (activated in workflow by setting
  # RUN_TASK_GET_OBS_CCPA=true). In the case of pulling observations 
  # directly from NOAA HPSS, the data retrieved will be placed in this 
  # directory. Please note, this path must be defind as 
  # /full-path-to-obs/ccpa/proc. METplus is configured to verify 01-, 
  # 03-, 06-, and 24-h accumulated precipitation using hourly CCPA files.
  # METplus configuration files require the use of predetermined directory 
  # structure and file names. Therefore, if the CCPA files are user 
  # provided, they need to follow the anticipated naming structure: 
  # {YYYYMMDD}/ccpa.t{HH}z.01h.hrap.conus.gb2, where YYYY is the 4-digit 
  # valid year, MM the 2-digit valid month, DD the 2-digit valid day of 
  # the month, and HH the 2-digit valid hour of the day. In addition, a 
  # caveat is noted for using hourly CCPA data. There is a problem with 
  # the valid time in the metadata for files valid from 19 - 00 UTC (or 
  # files under the '00' directory). The script to pull the CCPA data 
  # from the NOAA HPSS has an example of how to account for this as well
  # as organizing the data into a more intuitive format:
  # scripts/exregional_get_ccpa_files.sh. When a fix is provided, it will
  # be accounted for in the exregional_get_ccpa_files.sh script.
  #
  # MRMS_OBS_DIR:
  # User-specified location of top-level directory where MRMS composite
  # reflectivity files used by METplus are located.  This parameter needs
  # to be set for both user-provided observations and for observations
  # that are retrieved from the NOAA HPSS (if the user has access) via the
  # TN_GET_OBS_MRMS task (activated in workflow by setting
  # RUN_TASK_GET_OBS_MRMS=true).  In the case of pulling observations 
  # directly from NOAA HPSS, the data retrieved will be placed in this 
  # directory. Please note, this path must be defind as 
  # /full-path-to-obs/mrms/proc. METplus configuration files require the
  # use of predetermined directory structure and file names. Therefore, if
  # the MRMS files are user provided, they need to follow the anticipated 
  # naming structure:
  # {YYYYMMDD}/MergedReflectivityQCComposite_00.50_{YYYYMMDD}-{HH}{mm}{SS}.grib2,
  # where YYYY is the 4-digit valid year, MM the 2-digit valid month, DD 
  # the 2-digit valid day of the month, HH the 2-digit valid hour of the 
  # day, mm the 2-digit valid minutes of the hour, and SS is the two-digit
  # valid seconds of the hour. In addition, METplus is configured to look
  # for a MRMS composite reflectivity file for the valid time of the 
  # forecast being verified; since MRMS composite reflectivity files do 
  # not always exactly match the valid time, a script, within the main 
  # script to retrieve MRMS data from the NOAA HPSS, is used to identify
  # and rename the MRMS composite reflectivity file to match the valid
  # time of the forecast.  The script to pull the MRMS data from the NOAA 
  # HPSS has an example of the expected file naming structure: 
  # scripts/exregional_get_mrms_files.sh. This script calls the script
  # used to identify the MRMS file closest to the valid time:
  # ush/mrms_pull_topofhour.py.
  #
  # NDAS_OBS_DIR:
  # User-specified location of top-level directory where NDAS prepbufr 
  # files used by METplus are located. This parameter needs to be set for
  # both user-provided observations and for observations that are 
  # retrieved from the NOAA HPSS (if the user has access) via the 
  # TN_GET_OBS_NDAS task (activated in workflow by setting 
  # RUN_TASK_GET_OBS_NDAS=true). In the case of pulling observations 
  # directly from NOAA HPSS, the data retrieved will be placed in this 
  # directory. Please note, this path must be defind as 
  # /full-path-to-obs/ndas/proc. METplus is configured to verify 
  # near-surface variables hourly and upper-air variables at times valid 
  # at 00 and 12 UTC with NDAS prepbufr files.  METplus configuration files
  # require the use of predetermined file names. Therefore, if the NDAS 
  # files are user provided, they need to follow the anticipated naming 
  # structure: prepbufr.ndas.{YYYYMMDDHH}, where YYYY is the 4-digit valid
  # year, MM the 2-digit valid month, DD the 2-digit valid day of the 
  # month, and HH the 2-digit valid hour of the day. The script to pull 
  # the NDAS data from the NOAA HPSS has an example of how to rename the
  # NDAS data into a more intuitive format with the valid time listed in 
  # the file name: scripts/exregional_get_ndas_files.sh
  #
  #-----------------------------------------------------------------------
  #
  MET_INSTALL_DIR: ""
  MET_BIN_EXEC: ""
  METPLUS_PATH: ""
  CCPA_OBS_DIR: ""
  MRMS_OBS_DIR: ""
  NDAS_OBS_DIR: ""
  #
  #-----------------------------------------------------------------------
  #
  # DOMAIN_PREGEN_BASEDIR:
  # The base directory containing pregenerated grid, orography, and surface 
  # climatology files. This is an alternative for setting GRID_DIR,
  # OROG_DIR, and SFC_CLIMO_DIR individually
  # 
  # For the pregenerated grid specified by PREDEF_GRID_NAME, 
  # these "fixed" files are located in:
  #
  #   ${DOMAIN_PREGEN_BASEDIR}/${PREDEF_GRID_NAME}
  #
  # The workflow scripts will create a symlink in the experiment directory
  # that will point to a subdirectory (having the name of the grid being
  # used) under this directory.  This variable should be set to a null 
  # string in this file, but it can be specified in the user-specified 
  # workflow configuration file (EXPT_CONFIG_FN).
  #
  #-----------------------------------------------------------------------
  #
  DOMAIN_PREGEN_BASEDIR: ""
  #
  #-----------------------------------------------------------------------
  # Pre task commands such as "ulimit" needed by tasks
  #-----------------------------------------------------------------------
  #
  PRE_TASK_CMDS: ""
  #
  #-----------------------------------------------------------------------
  # Test directories used in run_WE2E script
  #-----------------------------------------------------------------------
  #
  TEST_EXTRN_MDL_SOURCE_BASEDIR: ""
  TEST_PREGEN_BASEDIR: ""
  TEST_ALT_EXTRN_MDL_SYSBASEDIR_ICS: ""
  TEST_ALT_EXTRN_MDL_SYSBASEDIR_LBCS: ""
  TEST_VX_FCST_INPUT_BASEDIR: ""
  #
  #-----------------------------------------------------------------------
  #
  # Set parameters associated with the fixed (i.e. static) files.  Definitions:
  #
  # FIXgsm:
  # System directory in which the majority of fixed (i.e. time-independent) 
  # files that are needed to run the FV3-LAM model are located
  #
  # FIXaer:
  # System directory where MERRA2 aerosol climatology files are located
  #
  # FIXlut:
  # System directory where the lookup tables for optics properties are located
  #
  # FIXorg:
  # System directory where orography data is located
  #
  # FIXsfc:
  # System directory where surface climatology data is located
  #
  #-----------------------------------------------------------------------
  #
  FIXgsm: ""
  FIXaer: ""
  FIXlut: ""
  FIXorg: ""
  FIXsfc: ""
  FIXshp: ""
  #
  #-----------------------------------------------------------------------
  #
  # EXTRN_MDL_DATA_STORES:
  # A list of data stores where the scripts should look for external model
  # data. The list is in priority order. If disk information is provided
  # via USE_USER_STAGED_EXTRN_FILES or a known location on the platform,
  # the disk location will be highest priority. Options are disk, hpss,
  # aws, and nomads.
  #
  #-----------------------------------------------------------------------
  #
  EXTRN_MDL_DATA_STORES: ""

#-----------------------------
# WORKFLOW config parameters
#-----------------------------
workflow:
  #
  #-----------------------------------------------------------------------
  #
  # Unique ID for workflow run that will be set in setup.py
  #
  #-----------------------------------------------------------------------
  #
  WORKFLOW_ID: ""
  #
  #-----------------------------------------------------------------------
  #
  # How to make links. Relative links by default. Empty string for
  # absolute paths in links.
  #
  #-----------------------------------------------------------------------
  #
  RELATIVE_LINK_FLAG: "--relative"
  #
  #-----------------------------------------------------------------------
  #
  # Set cron-associated parameters.  Definitions:
  #
  # USE_CRON_TO_RELAUNCH:
  # Flag that determines whether or not to add a line to the user's cron 
  # table to call the experiment launch script every CRON_RELAUNCH_INTVL_MNTS 
  # minutes.
  #
  # CRON_RELAUNCH_INTVL_MNTS:
  # The interval (in minutes) between successive calls of the experiment
  # launch script by a cron job to (re)launch the experiment (so that the
  # workflow for the experiment kicks off where it left off).
  #
  #-----------------------------------------------------------------------
  #
  USE_CRON_TO_RELAUNCH: false
  CRON_RELAUNCH_INTVL_MNTS: 3
  CRONTAB_LINE: ""
  LOAD_MODULES_RUN_TASK_FP: '{{ [user.USHdir, "load_modules_run_task.sh"]|path_join }}'

  #
  #-----------------------------------------------------------------------
  #
  # Set directories.  Definitions:
  #
  # EXPT_BASEDIR:
  # The base directory in which the experiment directory will be created.  
  # If this is not specified or if it is set to an empty string, it will
  # default to ${HOMEaqm}/../expt_dirs. If set to a relative path, the
  # path will be appended to the default value ${HOMEaqm}/../expt_dirs 
  #
  # EXPT_SUBDIR:
  # The name that the experiment directory (without the full path) will
  # have.  The full path to the experiment directory, which will be contained
  # in the variable EXPTDIR, will be:
  #
  #   EXPTDIR: "${EXPT_BASEDIR}/${EXPT_SUBDIR}"
  #
  # This cannot be empty.  If set to a null string here, it must be set to
  # a (non-empty) value in the user-defined experiment configuration file.
  #
  # EXEC_SUBDIR:
  # The name of the subdirectory of ufs-srweather-app where executables are
  # installed.
  #-----------------------------------------------------------------------
  #
  EXPT_BASEDIR: '' # This will be set in setup.py prior to extend_yaml() being called
  EXPT_SUBDIR: '{{ EXPT_SUBDIR }}'
  EXEC_SUBDIR: "exec"
  EXPTDIR: '{{ [workflow.EXPT_BASEDIR, workflow.EXPT_SUBDIR]|path_join }}'
  #
  #-----------------------------------------------------------------------
  #
  # Set the separator character(s) to use in the names of the grid, mosaic,
  # and orography fixed files.
  #
  # Ideally, the same separator should be used in the names of these fixed
  # files as the surface climatology fixed files (which always use a "."
  # as the separator), i.e. ideally, DOT_OR_USCORE should be set to "."
  #
  #-----------------------------------------------------------------------
  #
  DOT_OR_USCORE: "_"
  #
  #-----------------------------------------------------------------------
  #
  # Set file names.  Definitions:
  #
  # EXPT_CONFIG_FN:
  # Name of the user-specified configuration file for the forecast experiment.
  #
  # CONSTANTS_FN:
  # Name of the file containing definitions of various mathematical, physical, 
  # and SRW App contants.
  #
  # RGNL_GRID_NML_FN:
  # Name of file containing the namelist settings for the code that generates
  # a "ESGgrid" type of regional grid.
  #
  # FV3_NML_BASE_SUITE_FN:
  # Name of Fortran namelist file containing the forecast model's base suite
  # namelist, i.e. the portion of the namelist that is common to all physics
  # suites.
  #
  # FV3_NML_YAML_CONFIG_FN:
  # Name of YAML configuration file containing the forecast model's namelist
  # settings for various physics suites.
  #
  # FV3_NML_BASE_ENS_FN:
  # Name of Fortran namelist file containing the forecast model's base 
  # ensemble namelist, i.e. the the namelist file that is the starting point 
  # from which the namelist files for each of the enesemble members are
  # generated.
  #
  # FV3_EXEC_FN:
  # Name to use for the forecast model executable when it is copied from
  # the directory in which it is created in the build step to the executables
  # directory (EXECDIR; this is set during experiment generation).
  #
  # DIAG_TABLE_TMPL_FN:
  # Name of a template file that specifies the output fields of the
  # forecast model (ufs-weather-model: diag_table) followed by the name
  # of the ccpp_phys_suite.  Its default value is the name of the file
  # that the ufs weather model 
  # expects to read in.
  #
  # FIELD_TABLE_TMPL_FN:
  # Name of a template file that specifies the tracers in IC/LBC files of the 
  # forecast model (ufs-weather-mode: field_table) followed by [dot_ccpp_phys_suite]. 
  # Its default value is the name of the file that the ufs weather model expects 
  # to read in.
  #
  # MODEL_CONFIG_TMPL_FN:
  # Name of a template file that contains settings and configurations for the 
  # NUOPC/ESMF main component (ufs-weather-model: model_config). Its default 
  # value is the name of the file that the ufs weather model expects to read in.
  #
  # NEMS_CONFIG_TMPL_FN:
  # Name of a template file that contains information about the various NEMS 
  # components and their run sequence (ufs-weather-model: nems.configure). 
  # Its default value is the name of the file that the ufs weather model expects 
  # to read in.
  #
  # AQM_RC_TMPL_FN:
  # Template file name of resource file for NOAA Air Quality Model (AQM)
  #
  # FCST_MODEL:
  # Name of forecast model (default=ufs-weather-model)
  #
  # WFLOW_XML_FN:
  # Name of the rocoto workflow XML file that the experiment generation
  # script creates and that defines the workflow for the experiment.
  #
  # GLOBAL_VAR_DEFNS_FN:
  # Name of file (a shell script) containing the defintions of the primary 
  # experiment variables (parameters) defined in this default configuration 
  # script and in the user-specified configuration as well as secondary 
  # experiment variables generated by the experiment generation script.  
  # This file is sourced by many scripts (e.g. the J-job scripts corresponding 
  # to each workflow task) in order to make all the experiment variables 
  # available in those scripts.
  #
  # EXTRN_MDL_VAR_DEFNS_FN:
  # Name of file (a shell script) containing the defintions of variables
  # associated with the external model from which ICs or LBCs are generated.  This
  # file is created by the TN_GET_EXTRN_* task because the values of the variables
  # it contains are not known before this task runs.  The file is then sourced by
  # the TN_MAKE_ICS and TN_MAKE_LBCS tasks.
  #
  # WFLOW_LAUNCH_SCRIPT_FN:
  # Name of the script that can be used to (re)launch the experiment's rocoto
  # workflow.
  #
  # WFLOW_LAUNCH_LOG_FN:
  # Name of the log file that contains the output from successive calls to
  # the workflow launch script (WFLOW_LAUNCH_SCRIPT_FN).
  #
  #-----------------------------------------------------------------------
  #
  EXPT_CONFIG_FN: "config.yaml"
  CONSTANTS_FN: "constants.yaml"
  
  RGNL_GRID_NML_FN: "regional_grid.nml"
  
  FV3_NML_BASE_SUITE_FN: "input.nml.FV3"
  FV3_NML_YAML_CONFIG_FN: "FV3.input.yml"
  FV3_NML_BASE_ENS_FN: "input.nml.base_ens"
  FV3_NML_FN: "input.nml"
  FV3_EXEC_FN: "ufs_model"

  DATA_TABLE_FN: "data_table"
  DIAG_TABLE_FN: "diag_table"
  FIELD_TABLE_FN: "field_table"
  DIAG_TABLE_TMPL_FN: 'diag_table.{{ CCPP_PHYS_SUITE }}'
  FIELD_TABLE_TMPL_FN: 'field_table.{{ CCPP_PHYS_SUITE }}'
  MODEL_CONFIG_FN: "model_configure"
  NEMS_CONFIG_FN: "nems.configure"
  AQM_RC_FN: "aqm.rc"
  AQM_RC_TMPL_FN: "aqm.rc"

  FV3_NML_BASE_SUITE_FP: '{{ [user.PARMdir, FV3_NML_BASE_SUITE_FN]|path_join }}'
  FV3_NML_YAML_CONFIG_FP: '{{ [user.PARMdir, FV3_NML_YAML_CONFIG_FN]|path_join }}'
  FV3_NML_BASE_ENS_FP: '{{ [EXPTDIR, FV3_NML_BASE_ENS_FN]|path_join }}'
  DATA_TABLE_TMPL_FP: '{{ [user.PARMdir, DATA_TABLE_FN]|path_join }}'
  DIAG_TABLE_TMPL_FP: '{{ [user.PARMdir, DIAG_TABLE_TMPL_FN]|path_join }}'
  FIELD_TABLE_TMPL_FP: '{{ [user.PARMdir, FIELD_TABLE_TMPL_FN]|path_join }}'
  MODEL_CONFIG_TMPL_FP: '{{ [user.PARMdir, MODEL_CONFIG_FN]|path_join }}'
  NEMS_CONFIG_TMPL_FP: '{{ [user.PARMdir, NEMS_CONFIG_FN]|path_join }}'
  AQM_RC_TMPL_FP: '{{ [user.PARMdir, AQM_RC_TMPL_FN]|path_join }}'

  # These are staged in the exptdir at configuration time
  DATA_TABLE_FP: '{{ [EXPTDIR, DATA_TABLE_FN]|path_join }}'
  FIELD_TABLE_FP: '{{ [EXPTDIR, FIELD_TABLE_FN]|path_join }}'
  NEMS_CONFIG_FP: '{{ [EXPTDIR, NEMS_CONFIG_FN]|path_join }}'
  FV3_NML_FP: '{{ [EXPTDIR, FV3_NML_FN]|path_join }}'

  FCST_MODEL: "ufs-weather-model"
  WFLOW_XML_FN: "FV3LAM_wflow.xml"
  GLOBAL_VAR_DEFNS_FN: "var_defns.sh"
  EXTRN_MDL_VAR_DEFNS_FN: "extrn_mdl_var_defns"
  WFLOW_LAUNCH_SCRIPT_FN: "launch_FV3LAM_wflow.sh"
  WFLOW_LAUNCH_LOG_FN: "log.launch_FV3LAM_wflow"

  GLOBAL_VAR_DEFNS_FP: '{{ [EXPTDIR, GLOBAL_VAR_DEFNS_FN] |path_join }}'
  WFLOW_LAUNCH_SCRIPT_FP: '{{ [user.USHdir, WFLOW_LAUNCH_SCRIPT_FN] |path_join }}'
  WFLOW_LAUNCH_LOG_FP: '{{ [EXPTDIR, WFLOW_LAUNCH_LOG_FN] |path_join }}'
  #
  #-----------------------------------------------------------------------
  #
  # Set the fix file paths
  #
  # FIXdir:
  # Location where fix files will be stored for a given experiment
  #
  # FIXam:
  # Directory containing the fixed files (or symlinks) for various fields on
  # global grids (which are usually much coarser than the native FV3-LAM grid).
  #
  # FIXclim:
  # Directory containing the MERRA2 aerosol climatology data file and
  # lookup tables for optics properties
  #
  # FIXlam:
  # Directory containing the fixed files (or symlinks) for the grid,
  # orography, and surface climatology on the native FV3-LAM grid.
  #
  # THOMPSON_MP_CLIMO_FN and _FP:
  # Name and path of file that contains aerosol climatology data. It can
  # be used to generate approximate versions of the aerosol fields
  # needed by Thompson microphysics.  This file will be used to
  # generate such approximate aerosol fields in the ICs and LBCs if
  # Thompson MP is included in the physics suite and if the exteranl
  # model for ICs or LBCs does not already provide these fields.
  #
  #-----------------------------------------------------------------------
  #
  FIXdir: '{{ EXPTDIR if workflow_switches.RUN_TASK_MAKE_GRID else [user.HOMEaqm, "fix"]|path_join }}'
  FIXam: '{{ [FIXdir, "fix_am"]|path_join }}'
  FIXclim: '{{ [FIXdir, "fix_clim"]|path_join }}'
  FIXlam: '{{ [FIXdir, "fix_lam"]|path_join }}'

  THOMPSON_MP_CLIMO_FN: "Thompson_MP_MONTHLY_CLIMO.nc"
  THOMPSON_MP_CLIMO_FP: '{{ [FIXam, THOMPSON_MP_CLIMO_FN]|path_join }}'
  #
  #-----------------------------------------------------------------------
  #
  # Set CCPP-associated parameters.  Definitions:
  #
  # CCPP_PHYS_SUITE:
  # The physics suite that will run using CCPP (Common Community Physics
  # Package).  The choice of physics suite determines the forecast model's 
  # namelist file, the diagnostics table file, the field table file, and 
  # the XML physics suite definition file that are staged in the experiment 
  # directory or the cycle directories under it.
  #
  # *_FN and *_FP variables set the name and paths to the suite
  # definition files used for the experiment
  #-----------------------------------------------------------------------
  #
  CCPP_PHYS_SUITE: "FV3_GFS_v16"
  CCPP_PHYS_SUITE_FN: 'suite_{{ CCPP_PHYS_SUITE }}.xml'
  CCPP_PHYS_SUITE_IN_CCPP_FP: '{{ [user.UFS_WTHR_MDL_DIR, "FV3", "ccpp", "suites", CCPP_PHYS_SUITE_FN] |path_join }}'
  CCPP_PHYS_SUITE_FP: '{{ [workflow.EXPTDIR, CCPP_PHYS_SUITE_FN]|path_join }}'
  #
  #-----------------------------------------------------------------------
  #
  # Set the field dictionary file name and paths.
  #
  #-----------------------------------------------------------------------
  #
  FIELD_DICT_FN: "fd_nems.yaml"
  FIELD_DICT_IN_UWM_FP: '{{ [user.UFS_WTHR_MDL_DIR, "tests", "parm", FIELD_DICT_FN]|path_join }}'
  FIELD_DICT_FP: '{{ [workflow.EXPTDIR, FIELD_DICT_FN]|path_join }}'
  #
  #-----------------------------------------------------------------------
  #
  # Set GRID_GEN_METHOD.  This variable specifies the method to use to 
  # generate a regional grid in the horizontal.  The values that it can 
  # take on are:
  #
  # * "GFDLgrid":
  #   This setting will generate a regional grid by first generating a 
  #   "parent" global cubed-sphere grid and then taking a portion of tile
  #   6 of that global grid -- referred to in the grid generation scripts
  #   as "tile 7" even though it doesn't correspond to a complete tile --
  #   and using it as the regional grid.  Note that the forecast is run on
  #   only on the regional grid (i.e. tile 7, not tiles 1 through 6).
  #
  # * "ESGgrid":
  #   This will generate a regional grid using the map projection developed
  #   by Jim Purser of EMC.
  #
  # Note that:
  #
  # 1) If the experiment is using one of the predefined grids (i.e. if 
  #    PREDEF_GRID_NAME is set to the name of one of the valid predefined 
  #    grids), then GRID_GEN_METHOD will be reset to the value of 
  #    GRID_GEN_METHOD for that grid.  This will happen regardless of 
  #    whether or not GRID_GEN_METHOD is assigned a value in the user-
  #    specified experiment configuration file, i.e. any value it may be
  #    assigned in the experiment configuration file will be overwritten.
  #
  # 2) If the experiment is not using one of the predefined grids (i.e. if 
  #    PREDEF_GRID_NAME is set to a null string), then GRID_GEN_METHOD must 
  #    be set in the experiment configuration file.  Otherwise, it will 
  #    remain set to a null string, and the experiment generation will 
  #    fail because the generation scripts check to ensure that it is set 
  #    to a non-empty string before creating the experiment directory.
  #
  #-----------------------------------------------------------------------
  #
  GRID_GEN_METHOD: ""
  #
  #-----------------------------------------------------------------------
  #
  # Set PREDEF_GRID_NAME.  This parameter specifies a predefined regional
  # grid, as follows:
  #
  # * If PREDEF_GRID_NAME is set to a valid predefined grid name, the grid 
  #   generation method GRID_GEN_METHOD, the (native) grid parameters, and 
  #   the write-component grid parameters are set to predefined values for 
  #   the specified grid, overwriting any settings of these parameters in 
  #   the user-specified experiment configuration file.  In addition, if 
  #   the time step DT_ATMOS and the computational parameters LAYOUT_X, 
  #   LAYOUT_Y, and BLOCKSIZE are not specified in that configuration file, 
  #   they are also set to predefined values for the specified grid.
  #
  # * If PREDEF_GRID_NAME is set to an empty string, it implies the user
  #   is providing the native grid parameters in the user-specified 
  #   experiment configuration file (EXPT_CONFIG_FN).  In this case, the 
  #   grid generation method GRID_GEN_METHOD, the native grid parameters, 
  #   and the write-component grid parameters as well as the time step 
  #   forecast model's main time step DT_ATMOS and the computational 
  #   parameters LAYOUT_X, LAYOUT_Y, and BLOCKSIZE must be set in that 
  #   configuration file; otherwise, the values of all of these parameters 
  #   in this default experiment configuration file will be used.
  #
  # Setting PREDEF_GRID_NAME provides a convenient method of specifying a
  # commonly used set of grid-dependent parameters.  The predefined grid 
  # parameters are specified in the script 
  #
  #   $HOMEaqm/ush/set_predef_grid_params.py
  #
  #-----------------------------------------------------------------------
  #
  PREDEF_GRID_NAME: ""
  #
  #-----------------------------------------------------------------------
  #
  # Set forecast parameters.  Definitions:
  #
  # DATE_FIRST_CYCL:
  # Starting cycle date of the FIRST forecast in the set of forecasts to
  # run.  Format is "YYYYMMDDHH". Note: This has recently changed to
  # include the first cycle hour.
  #
  # DATE_LAST_CYCL:
  # Starting cylce date of the LAST forecast in the set of forecasts to run.
  # Format is "YYYYMMDDHH".  Note: This has recently changed to include
  # the last cycle hour.
  #
  # INCR_CYCL_FREQ:
  # Increment in hours for Rocoto cycle frequency.
  # Default is 24, which means cycle_freq=24:00:00
  #
  # FCST_LEN_HRS:
  # The length of each forecast, in integer hours.
  #
  # FCST_LEN_CYCL:
  # The length of forecast for each cycle date in integer hours.
  # This is valid only when FCST_LEN_HRS = -1.
  # This pattern is recurred for all cycle dates.
  #
  #-----------------------------------------------------------------------
  #
  DATE_FIRST_CYCL: "YYYYMMDDHH"
  DATE_LAST_CYCL: "YYYYMMDDHH"
  INCR_CYCL_FREQ: 24
  FCST_LEN_HRS: 24
  FCST_LEN_CYCL:
    - '{{ FCST_LEN_HRS }}'

  #
  #-----------------------------------------------------------------------
  #
  # Set PREEXISTING_DIR_METHOD.  This variable determines the method to use
  # use to deal with preexisting directories [e.g ones generated by previous
  # calls to the experiment generation script using the same experiment name
  # (EXPT_SUBDIR) as the current experiment].  This variable must be set to
  # one of "delete", "rename", and "quit".  The resulting behavior for each
  # of these values is as follows:
  #
  # * "delete":
  #   The preexisting directory is deleted and a new directory (having the
  #   same name as the original preexisting directory) is created.
  #
  # * "rename":
  #   The preexisting directory is renamed and a new directory (having the
  #   same name as the original preexisting directory) is created.  The new
  #   name of the preexisting directory consists of its original name and
  #   the suffix "_oldNNN", where NNN is a 3-digit integer chosen to make
  #   the new name unique.
  #
  # * "quit":
  #   The preexisting directory is left unchanged, but execution of the
  #   currently running script is terminated.  In this case, the preexisting
  #   directory must be dealt with manually before rerunning the script.
  #
  #-----------------------------------------------------------------------
  #
  PREEXISTING_DIR_METHOD: "delete"
  #
  #-----------------------------------------------------------------------
  #
  # Set flags for more detailed messages.  Defintitions:
  #
  # VERBOSE:
  # This is a flag that determines whether or not the experiment generation 
  # and workflow task scripts tend to print out more informational messages.
  #
  # DEBUG:
  # This is a flag that determines whether or not very detailed debugging
  # messages are printed to out.  Note that if DEBUG is set to TRUE, then
  # VERBOSE will also get reset to TRUE if it isn't already.
  #
  #-----------------------------------------------------------------------
  #
  VERBOSE: true
  DEBUG: false
  #
  #-----------------------------------------------------------------------
  #
  # COMPILER:
  # Type of compiler invoked during the build step. Currently, this must 
  # be set manually; it is not inherited from the build system in the 
  # ufs-srweather-app directory.
  #
  # SYMLINK_FIX_FILES:
  # Symlink fix files to experiment directory if true; otherwise copy the files.
  #
  #------------------------------------------------------------------------
  #
  COMPILER: "intel"
  SYMLINK_FIX_FILES: true
  #
  #-----------------------------------------------------------------------
  #
  # DO_REAL_TIME:
  # switch for real-time run
  #
  #-----------------------------------------------------------------------
  #
  DO_REAL_TIME: false
  #
  #-----------------------------------------------------------------------
  #
  # COLDSTART:
  # Flag turning on/off warm start of the first cycle
  #
  # WARMSTART_CYCLE_DIR:
  # Path to the directory where RESTART dir is located for warm start
  #
  #-----------------------------------------------------------------------
  #
  COLDSTART: true
  WARMSTART_CYCLE_DIR: "/path/to/warm/start/cycle/dir"

#----------------------------
# NCO specific variables
#-----------------------------
nco:
  #
  #-----------------------------------------------------------------------
  #
  # Set variables that are only used in NCO mode (i.e. when RUN_ENVIR is 
  # set to "nco"). All variables have the suffix _dfv meaning the default value.
  # This is bacuase they are used as the default values for the production using ecFlow.
  #
  # Definitions:
  #
  # envir, NET, model_ver, RUN:
  # Standard environment variables defined in the NCEP Central Operations WCOSS
  # Implementation Standards document as follows:
  #
  #   envir:
  #   Set to "test" during the initial testing phase, "para" when running
  #   in parallel (on a schedule), and "prod" in production.
  #
  #   NET:
  #   Model name (first level of com directory structure)
  #
  #   model_ver:
  #   Version number of package in three digits (second level of com directory)
  #
  #   RUN:
  #   Name of model run (third level of com directory structure).
  #   In general, same as $NET
  #
  # OPSROOT:
  # The operations root directory in NCO mode.
  #
  # LOGBASEDIR:
  # Directory in which the log files from the workflow tasks will be placed.
  # 
  # For more information on NCO standards
  #   
  #   https://www.nco.ncep.noaa.gov/idsb/implementation_standards/ImplementationStandards.v11.0.0.pdf
  #
  #-----------------------------------------------------------------------
  #
  envir_dfv: "prod"
  NET_dfv: "aqm"
  RUN_dfv: "aqm"
  model_ver_dfv: "v7.0.0"
  OPSROOT_dfv: '{{ workflow.EXPT_BASEDIR }}/../nco_dirs'
  COMROOT_dfv: '{{ OPSROOT_dfv }}/com'
  COMIN_BASEDIR: '{{ COMROOT_dfv }}/{{ NET_dfv }}/{{ model_ver_dfv }}'
  COMOUT_BASEDIR: '{{ COMROOT_dfv }}/{{ NET_dfv }}/{{ model_ver_dfv }}'

  DATAROOT_dfv: '{{ OPSROOT_dfv }}/tmp'
  DCOMROOT_dfv: '{{ OPSROOT_dfv }}/dcom'
  LOGBASEDIR_dfv: '{{ OPSROOT_dfv }}/output'

  #
  #-----------------------------------------------------------------------
  #
  # The following are also described in the NCO doc above: default values
  #
  #-----------------------------------------------------------------------
  #
  DBNROOT_dfv: ""
  SENDECF_dfv: false
  SENDDBN_dfv: false
  SENDDBN_NTC_dfv: false
  SENDCOM_dfv: false
  SENDWEB_dfv: false
  KEEPDATA_dfv: true
  MAILTO_dfv: ""
  MAILCC_dfv: ""


#----------------------------
# WORKFLOW SWITCHES config parameters
#-----------------------------
workflow_switches:
  #
  #-----------------------------------------------------------------------
  #
  # Set flags (and related directories) that determine whether various
  # workflow tasks should be run.  Note that the TN_MAKE_GRID, TN_MAKE_OROG, 
  # and TN_MAKE_SFC_CLIMO are all cycle-independent tasks, i.e. if they 
  # are to be run, they do so only once at the beginning of the workflow 
  # before any cycles are run.  Definitions:
  #
  # RUN_TASK_MAKE_GRID:
  # Flag that determines whether the TN_MAKE_GRID task is to be run.  If 
  # this is set to true, the grid generation task is run and new grid
  # files are generated.  If it is set to false, then the scripts look
  # for pregenerated grid files in the directory specified by GRID_DIR 
  # (see below).
  #
  # RUN_TASK_MAKE_OROG:
  # Same as RUN_TASK_MAKE_GRID but for the TN_MAKE_OROG task.
  #
  # RUN_TASK_MAKE_SFC_CLIMO:
  # Same as RUN_TASK_MAKE_GRID but for the TN_MAKE_SFC_CLIMO task.
  #
  # RUN_TASK_GET_EXTRN_ICS:
  # Flag that determines whether the TN_GET_EXTRN_ICS task is to be run.
  #
  # RUN_TASK_GET_EXTRN_LBCS:
  # Flag that determines whether the TN_GET_EXTRN_LBCS task is to be run.
  #
  # RUN_TASK_MAKE_ICS:
  # Flag that determines whether the TN_MAKE_ICS task is to be run.
  #
  # RUN_TASK_MAKE_LBCS:
  # Flag that determines whether the TN_MAKE_LBCS task is to be run.
  #
  # RUN_TASK_RUN_FCST:
  # Flag that determines whether the TN_RUN_FCST task is to be run.
  #
  # RUN_TASK_RUN_POST:
  # Flag that determines whether the TN_RUN_POST task is to be run.
  # 
  # RUN_TASK_RUN_PRDGEN:
  # Flag that determines whether the TN_RUN_PRDGEN task is to be run.
  #
  # RUN_TASK_GET_OBS_CCPA:
  # Flag that determines whether to run the TN_GET_OBS_CCPA task, which
  # retrieves the CCPA hourly precipitation files used by METplus from NOAA HPSS. 
  # 
  # RUN_TASK_GET_OBS_MRMS:
  # Flag that determines whether to run the TN_GET_OBS_MRMS task, which
  # retrieves the MRMS composite reflectivity files used by METplus from NOAA HPSS. 
  #
  # RUN_TASK_GET_OBS_NDAS:
  # Flag that determines whether to run the TN_GET_OBS_NDAS task, which
  # retrieves the NDAS PrepBufr files used by METplus from NOAA HPSS. 
  #
  # RUN_TASK_VX_GRIDSTAT:
  # Flag that determines whether the grid-stat verification task is to be
  # run.
  #
  # RUN_TASK_VX_POINTSTAT:
  # Flag that determines whether the point-stat verification task is to be
  # run.
  #
  # RUN_TASK_VX_ENSGRID:
  # Flag that determines whether the ensemble-stat verification for gridded
  # data task is to be run. 
  #
  # RUN_TASK_VX_ENSPOINT:
  # Flag that determines whether the ensemble point verification task is
  # to be run. If this flag is set, both ensemble-stat point verification
  # and point verification of ensemble-stat output is computed.
  #
  # RUN_TASK_PLOT_ALLVARS:
  # Flag that determines whether to run python plotting scripts
  #
  # RUN_TASK_AQM_ICS:
  # Flag that determines whether the TN_AQM_ICS task is to be run for air quality modeling.
  #
  # RUN_TASK_AQM_LBCS:
  # Flag that determines whether the TN_AQM_LBCS task is to be run for air quality modeling.
  #
  # RUN_TASK_NEXUS_GFS_SFC:
  # Flag that determines whether the TN_NEXUS_GFS_SFC task is to be run for air quality modeling.
  #
  # RUN_TASK_NEXUS_EMISSION:
  # Flag that determines whether the TN_NEXUS_EMISSION task is to be run for air quality modeling.
  #
  # RUN_TASK_FIRE_EMISSION:
  # Flag that determines whether the TN_FIRE_EMISSION task is to be run for air quality modeling.
  #
  # RUN_TASK_POINT_SOURCE:
  # Flag that determines whether the TN_POINT_SOURCE task is to be run for air quality modeling.
  #
  # RUN_TASK_PRE_POST_STAT:
  # Flag that determines whether the TN_PRE_POST_STAT task is to be run for air quality modeling.
  #
  # RUN_TASK_POST_STAT_O3:
  # Flag that determines whether the TN_POST_STAT_O3 task is to be run for air quality modeling.
  #
  # RUN_TASK_POST_STAT_PM25:
  # Flag that determines whether the TN_POST_STAT_PM25 task is to be run for air quality modeling.
  #
  # RUN_TASK_BIAS_CORRECTION_O3:
  # Flag that determines whether the TN_BIAS_CORRECTION_O3 task is to be run for air quality modeling.
  #
  # RUN_TASK_BIAS_CORRECTION_PM25:
  # Flag that determines whether the TN_BIAS_CORRECTION_PM25 task is to be run for air quality modeling.
  #
  #-----------------------------------------------------------------------
  #
  RUN_TASK_MAKE_GRID: true
  RUN_TASK_MAKE_OROG: true
  RUN_TASK_MAKE_SFC_CLIMO: true
  
  RUN_TASK_GET_EXTRN_ICS: true
  RUN_TASK_GET_EXTRN_LBCS: true
  RUN_TASK_MAKE_ICS: true
  RUN_TASK_MAKE_LBCS: true
  RUN_TASK_RUN_FCST: true
  RUN_TASK_RUN_POST: true

  RUN_TASK_RUN_PRDGEN: false
  
  RUN_TASK_GET_OBS_CCPA: false
  RUN_TASK_GET_OBS_MRMS: false
  RUN_TASK_GET_OBS_NDAS: false
  RUN_TASK_VX_GRIDSTAT: false
  RUN_TASK_VX_POINTSTAT: false
  RUN_TASK_VX_ENSGRID: false
  RUN_TASK_VX_ENSPOINT: false

  RUN_TASK_PLOT_ALLVARS: false

  RUN_TASK_AQM_ICS: false
  RUN_TASK_AQM_LBCS: false
  RUN_TASK_NEXUS_GFS_SFC: false
  RUN_TASK_NEXUS_EMISSION: false
  RUN_TASK_FIRE_EMISSION: false
  RUN_TASK_POINT_SOURCE: false
  RUN_TASK_PRE_POST_STAT: false
  RUN_TASK_POST_STAT_O3: false
  RUN_TASK_POST_STAT_PM25: false
  RUN_TASK_BIAS_CORRECTION_O3: false
  RUN_TASK_BIAS_CORRECTION_PM25: false


#----------------------------
# MAKE GRID config parameters
#-----------------------------
task_make_grid:
  TN_MAKE_GRID: "make_grid"
  NNODES_MAKE_GRID: 1
  PPN_MAKE_GRID: 24
  WTIME_MAKE_GRID: 00:20:00
  MAXTRIES_MAKE_GRID: 2
  #
  #-----------------------------------------------------------------------
  #
  # GRID_DIR:
  # The directory in which to look for pregenerated grid files if 
  # RUN_TASK_MAKE_GRID is set to false.
  # 
  #-----------------------------------------------------------------------
  # 
  GRID_DIR: '{{ [workflow.EXPTDIR, "grid"]|path_join if workflow_switches.RUN_TASK_MAKE_GRID else "" }}'
  #
  #-----------------------------------------------------------------------
  #
  # Set parameters specific to the "ESGgrid" method of generating a regional
  # grid (i.e. for GRID_GEN_METHOD set to "ESGgrid").  Definitions:
  #
  # ESGgrid_LON_CTR:
  # The longitude of the center of the grid (in degrees).
  #
  # ESGgrid_LAT_CTR:
  # The latitude of the center of the grid (in degrees).
  #
  # ESGgrid_DELX:
  # The cell size in the zonal direction of the regional grid (in meters).
  #
  # ESGgrid_DELY:
  # The cell size in the meridional direction of the regional grid (in 
  # meters).
  #
  # ESGgrid_NX:
  # The number of cells in the zonal direction on the regional grid.
  #
  # ESGgrid_NY:
  # The number of cells in the meridional direction on the regional grid.
  #
  # ESGgrid_WIDE_HALO_WIDTH:
  # The width (in units of number of grid cells) of the halo to add around
  # the regional grid before shaving the halo down to the width(s) expected
  # by the forecast model.  
  #
  # ESGgrid_PAZI:
  # The rotational parameter for the ESG grid (in degrees).
  #
  # In order to generate grid files containing halos that are 3-cell and
  # 4-cell wide and orography files with halos that are 0-cell and 3-cell
  # wide (all of which are required as inputs to the forecast model), the
  # grid and orography tasks first create files with halos around the regional
  # domain of width ESGgrid_WIDE_HALO_WIDTH cells.  These are first stored 
  # in files.  The files are then read in and "shaved" down to obtain grid
  # files with 3-cell-wide and 4-cell-wide halos and orography files with
  # 0-cell-wide (i.e. no halo) and 3-cell-wide halos.  For this reason, we
  # refer to the original halo that then gets shaved down as the "wide" 
  # halo, i.e. because it is wider than the 0-cell-wide, 3-cell-wide, and
  # 4-cell-wide halos that we will eventually end up with.  Note that the
  # grid and orography files with the wide halo are only needed as intermediates
  # in generating the files with 0-cell-, 3-cell-, and 4-cell-wide halos;
  # they are not needed by the forecast model.  
  # NOTE: Probably don't need to make ESGgrid_WIDE_HALO_WIDTH a user-specified 
  #       variable.  Just set it in the function set_gridparams_ESGgrid.py.
  #
  # Note that:
  #
  # 1) If the experiment is using one of the predefined grids (i.e. if 
  #    PREDEF_GRID_NAME is set to the name of one of the valid predefined
  #    grids), then:
  #
  #    a) If the value of GRID_GEN_METHOD for that grid is "GFDLgrid", then
  #       these parameters will not be used and thus do not need to be reset
  #       to non-empty strings.
  #
  #    b) If the value of GRID_GEN_METHOD for that grid is "ESGgrid", then
  #       these parameters will get reset to the values for that grid.  
  #       This will happen regardless of whether or not they are assigned 
  #       values in the user-specified experiment configuration file, i.e. 
  #       any values they may be assigned in the experiment configuration 
  #       file will be overwritten.
  #
  # 2) If the experiment is not using one of the predefined grids (i.e. if 
  #    PREDEF_GRID_NAME is set to a null string), then:
  #
  #    a) If GRID_GEN_METHOD is set to "GFDLgrid" in the user-specified 
  #       experiment configuration file, then these parameters will not be 
  #       used and thus do not need to be reset to non-empty strings.
  #
  #    b) If GRID_GEN_METHOD is set to "ESGgrid" in the user-specified 
  #       experiment configuration file, then these parameters must be set
  #       in that configuration file.
  #
  #-----------------------------------------------------------------------
  #
  ESGgrid_LON_CTR: ""
  ESGgrid_LAT_CTR: ""
  ESGgrid_DELX: ""
  ESGgrid_DELY: ""
  ESGgrid_NX: ""
  ESGgrid_NY: ""
  ESGgrid_WIDE_HALO_WIDTH: ""
  ESGgrid_PAZI: ""

  #-----------------------------------------------------------------------
  #
  # Set parameters specific to the "GFDLgrid" method of generating a regional
  # grid (i.e. for GRID_GEN_METHOD set to "GFDLgrid").  The following 
  # parameters will be used only if GRID_GEN_METHOD is set to "GFDLgrid". 
  # In this grid generation method:
  #
  # * The regional grid is defined with respect to a "parent" global cubed-
  #   sphere grid.  Thus, all the parameters for a global cubed-sphere grid
  #   must be specified in order to define this parent global grid even 
  #   though the model equations are not integrated on (they are integrated
  #   only on the regional grid).
  #
  # * GFDLgrid_NUM_CELLS is the number of grid cells in either one of the two 
  #   horizontal directions x and y on any one of the 6 tiles of the parent
  #   global cubed-sphere grid.  The mapping from GFDLgrid_NUM_CELLS to a nominal
  #   resolution (grid cell size) for a uniform global grid (i.e. Schmidt
  #   stretch factor GFDLgrid_STRETCH_FAC set to 1) for several values of
  #   GFDLgrid_NUM_CELLS is as follows:
  #
  #     GFDLgrid_NUM_CELLS      typical cell size
  #     ------------      -----------------
  #              192                  50 km
  #              384                  25 km
  #              768                  13 km
  #             1152                 8.5 km
  #             3072                 3.2 km
  #
  #   Note that these are only typical cell sizes.  The actual cell size on
  #   the global grid tiles varies somewhat as we move across a tile.
  #
  # * Tile 6 has arbitrarily been chosen as the tile to use to orient the
  #   global parent grid on the sphere (Earth).  This is done by specifying 
  #   GFDLgrid_LON_T6_CTR and GFDLgrid_LAT_T6_CTR, which are the longitude
  #   and latitude (in degrees) of the center of tile 6.
  #
  # * Setting the Schmidt stretching factor GFDLgrid_STRETCH_FAC to a value
  #   greater than 1 shrinks tile 6, while setting it to a value less than 
  #   1 (but still greater than 0) expands it.  The remaining 5 tiles change
  #   shape as necessary to maintain global coverage of the grid.
  #
  # * The cell size on a given global tile depends on both GFDLgrid_NUM_CELLS and
  #   GFDLgrid_STRETCH_FAC (since changing GFDLgrid_NUM_CELLS changes the number
  #   of cells in the tile, and changing GFDLgrid_STRETCH_FAC modifies the
  #   shape and size of the tile).
  #
  # * The regional grid is embedded within tile 6 (i.e. it doesn't extend
  #   beyond the boundary of tile 6).  Its exact location within tile 6 is
  #   is determined by specifying the starting and ending i and j indices
  #   of the regional grid on tile 6, where i is the grid index in the x
  #   direction and j is the grid index in the y direction.  These indices
  #   are stored in the variables 
  #
  #     GFDLgrid_ISTART_OF_RGNL_DOM_ON_T6G
  #     GFDLgrid_JSTART_OF_RGNL_DOM_ON_T6G
  #     GFDLgrid_IEND_OF_RGNL_DOM_ON_T6G
  #     GFDLgrid_JEND_OF_RGNL_DOM_ON_T6G
  #
  # * In the forecast model code and in the experiment generation and workflow
  #   scripts, for convenience the regional grid is denoted as "tile 7" even
  #   though it doesn't map back to one of the 6 faces of the cube from 
  #   which the parent global grid is generated (it maps back to only a 
  #   subregion on face 6 since it is wholly confined within tile 6).  Tile
  #   6 may be referred to as the "parent" tile of the regional grid.
  #
  # * GFDLgrid_REFINE_RATIO is the refinement ratio of the regional grid 
  #   (tile 7) with respect to the grid on its parent tile (tile 6), i.e.
  #   it is the number of grid cells along the boundary of the regional grid
  #   that abut one cell on tile 6.  Thus, the cell size on the regional 
  #   grid depends not only on GFDLgrid_NUM_CELLS and GFDLgrid_STRETCH_FAC (because
  #   the cell size on tile 6 depends on these two parameters) but also on 
  #   GFDLgrid_REFINE_RATIO.  Note that as on the tiles of the global grid, 
  #   the cell size on the regional grid is not uniform but varies as we 
  #   move across the grid.
  #
  # Definitions of parameters that need to be specified when GRID_GEN_METHOD
  # is set to "GFDLgrid":
  #
  # GFDLgrid_LON_T6_CTR:
  # Longitude of the center of tile 6 (in degrees).
  #
  # GFDLgrid_LAT_T6_CTR:
  # Latitude of the center of tile 6 (in degrees).
  #
  # GFDLgrid_NUM_CELLS:
  # Number of points in each of the two horizontal directions (x and y) on
  # each tile of the parent global grid.  Note that the name of this parameter
  # is really a misnomer because although it has the string "RES" (for 
  # "resolution") in its name, it specifies number of grid cells, not grid
  # size (in say meters or kilometers).  However, we keep this name in order
  # to remain consistent with the usage of the word "resolution" in the 
  # global forecast model and other auxiliary codes.
  #
  # GFDLgrid_STRETCH_FAC:
  # Stretching factor used in the Schmidt transformation applied to the
  # parent cubed-sphere grid.
  #
  # GFDLgrid_REFINE_RATIO:
  # Cell refinement ratio for the regional grid, i.e. the number of cells
  # in either the x or y direction on the regional grid (tile 7) that abut
  # one cell on its parent tile (tile 6).
  #
  # GFDLgrid_ISTART_OF_RGNL_DOM_ON_T6G:
  # i-index on tile 6 at which the regional grid (tile 7) starts.
  #
  # GFDLgrid_IEND_OF_RGNL_DOM_ON_T6G:
  # i-index on tile 6 at which the regional grid (tile 7) ends.
  #
  # GFDLgrid_JSTART_OF_RGNL_DOM_ON_T6G:
  # j-index on tile 6 at which the regional grid (tile 7) starts.
  #
  # GFDLgrid_JEND_OF_RGNL_DOM_ON_T6G:
  # j-index on tile 6 at which the regional grid (tile 7) ends.
  #
  # GFDLgrid_USE_NUM_CELLS_IN_FILENAMES:
  # Flag that determines the file naming convention to use for grid, orography,
  # and surface climatology files (or, if using pregenerated files, the
  # naming convention that was used to name these files).  These files 
  # usually start with the string "C${RES}_", where RES is an integer.
  # In the global forecast model, RES is the number of points in each of
  # the two horizontal directions (x and y) on each tile of the global grid
  # (defined here as GFDLgrid_NUM_CELLS).  If this flag is set to true, RES will
  # be set to GFDLgrid_NUM_CELLS just as in the global forecast model.  If it is
  # set to false, we calculate (in the grid generation task) an "equivalent
  # global uniform cubed-sphere resolution" -- call it RES_EQUIV -- and 
  # then set RES equal to it.  RES_EQUIV is the number of grid points in 
  # each of the x and y directions on each tile that a global UNIFORM (i.e. 
  # stretch factor of 1) cubed-sphere grid would have to have in order to
  # have the same average grid size as the regional grid.  This is a more
  # useful indicator of the grid size because it takes into account the 
  # effects of GFDLgrid_NUM_CELLS, GFDLgrid_STRETCH_FAC, and GFDLgrid_REFINE_RATIO
  # in determining the regional grid's typical grid size, whereas simply
  # setting RES to GFDLgrid_NUM_CELLS doesn't take into account the effects of
  # GFDLgrid_STRETCH_FAC and GFDLgrid_REFINE_RATIO on the regional grid's
  # resolution.  Nevertheless, some users still prefer to use GFDLgrid_NUM_CELLS
  # in the file names, so we allow for that here by setting this flag to
  # true.
  #
  # Note that:
  #
  # 1) If the experiment is using one of the predefined grids (i.e. if 
  #    PREDEF_GRID_NAME is set to the name of one of the valid predefined
  #    grids), then:
  #
  #    a) If the value of GRID_GEN_METHOD for that grid is "GFDLgrid", then
  #       these parameters will get reset to the values for that grid.  
  #       This will happen regardless of whether or not they are assigned 
  #       values in the user-specified experiment configuration file, i.e. 
  #       any values they may be assigned in the experiment configuration 
  #       file will be overwritten.
  #
  #    b) If the value of GRID_GEN_METHOD for that grid is "ESGgrid", then
  #       these parameters will not be used and thus do not need to be reset
  #       to non-empty strings.
  #
  # 2) If the experiment is not using one of the predefined grids (i.e. if 
  #    PREDEF_GRID_NAME is set to a null string), then:
  #
  #    a) If GRID_GEN_METHOD is set to "GFDLgrid" in the user-specified 
  #       experiment configuration file, then these parameters must be set
  #       in that configuration file.
  #
  #    b) If GRID_GEN_METHOD is set to "ESGgrid" in the user-specified 
  #       experiment configuration file, then these parameters will not be 
  #       used and thus do not need to be reset to non-empty strings.
  #
  #-----------------------------------------------------------------------
  #
  GFDLgrid_LON_T6_CTR: ""
  GFDLgrid_LAT_T6_CTR: ""
  GFDLgrid_NUM_CELLS: ""
  GFDLgrid_STRETCH_FAC: ""
  GFDLgrid_REFINE_RATIO: ""
  GFDLgrid_ISTART_OF_RGNL_DOM_ON_T6G: ""
  GFDLgrid_IEND_OF_RGNL_DOM_ON_T6G: ""
  GFDLgrid_JSTART_OF_RGNL_DOM_ON_T6G: ""
  GFDLgrid_JEND_OF_RGNL_DOM_ON_T6G: ""
  GFDLgrid_USE_NUM_CELLS_IN_FILENAMES: ""
  #
#----------------------------
# MAKE OROG config parameters
#-----------------------------
task_make_orog:
  TN_MAKE_OROG: "make_orog"
  NNODES_MAKE_OROG: 1
  PPN_MAKE_OROG: 24
  WTIME_MAKE_OROG: 00:20:00
  MAXTRIES_MAKE_OROG: 2
  KMP_AFFINITY_MAKE_OROG: "disabled"
  OMP_NUM_THREADS_MAKE_OROG: 6
  OMP_STACKSIZE_MAKE_OROG: "2048m"
  OROG_DIR: '{{ [workflow.EXPTDIR, "orog"]|path_join if workflow_switches.RUN_TASK_MAKE_OROG else "" }}'

#----------------------------
# MAKE SFC CLIMO config parameters
#-----------------------------
task_make_sfc_climo:
  TN_MAKE_SFC_CLIMO: "make_sfc_climo"
  NNODES_MAKE_SFC_CLIMO: 2
  PPN_MAKE_SFC_CLIMO: 24
  WTIME_MAKE_SFC_CLIMO: 00:20:00
  MAXTRIES_MAKE_SFC_CLIMO: 2
  KMP_AFFINITY_MAKE_SFC_CLIMO: "scatter"
  OMP_NUM_THREADS_MAKE_SFC_CLIMO: 1
  OMP_STACKSIZE_MAKE_SFC_CLIMO: "1024m"
  SFC_CLIMO_DIR: '{{ [workflow.EXPTDIR, "sfc_climo"]|path_join if workflow_switches.RUN_TASK_MAKE_SFC_CLIMO else "" }}'

#----------------------------
# EXTRN ICS config parameters
#-----------------------------
task_get_extrn_ics:
  TN_GET_EXTRN_ICS: "get_extrn_ics"
  NNODES_GET_EXTRN_ICS: 1
  PPN_GET_EXTRN_ICS: 1
  MEM_GET_EXTRN_ICS: 2G
  WTIME_GET_EXTRN_ICS: 00:45:00
  MAXTRIES_GET_EXTRN_ICS: 1
  #
  #-----------------------------------------------------------------------
  #
  # Set initial and lateral boundary condition generation parameters.  
  # Definitions:
  #
  # EXTRN_MDL_NAME_ICS:
  #`The name of the external model that will provide fields from which 
  # initial condition (including and surface) files will be generated for
  # input into the forecast model.
  #
  # EXTRN_MDL_ICS_OFFSET_HRS:
  # Users may wish to start a forecast from a forecast of a previous cycle
  # of an external model. This variable sets the number of hours earlier
  # the external model started than when the FV3 forecast configured here
  # should start. For example, the forecast should start from a 6 hour
  # forecast of the GFS, then EXTRN_MDL_ICS_OFFSET_HRS=6.
  #
  # FV3GFS_FILE_FMT_ICS:
  # If using the FV3GFS model as the source of the ICs (i.e. if EXTRN_MDL_NAME_ICS
  # is set to "FV3GFS"), this variable specifies the format of the model
  # files to use when generating the ICs.
  #
  #-----------------------------------------------------------------------
  #
  EXTRN_MDL_NAME_ICS: "FV3GFS"
  EXTRN_MDL_ICS_OFFSET_HRS: 0
  FV3GFS_FILE_FMT_ICS: "nemsio"
  #
  #-----------------------------------------------------------------------
  #
  # Base directories in which to search for external model files.
  #
  # EXTRN_MDL_SYSBASEDIR_ICS:
  # Base directory on the local machine containing external model files for
  # generating ICs on the native grid.  The way the full path containing 
  # these files is constructed depends on the user-specified external model
  # for ICs, i.e. EXTRN_MDL_NAME_ICS.
  #
  # Note that this must be defined as a null string here so that if it is 
  # specified by the user in the experiment configuration file, it remains 
  # set to those values, and if not, it gets set to machine-dependent 
  # values.
  #
  #-----------------------------------------------------------------------
  # 
  EXTRN_MDL_SYSBASEDIR_ICS: ''
  #
  #-----------------------------------------------------------------------
  #
  # User-staged external model directories and files.  Definitions:
  #
  # USE_USER_STAGED_EXTRN_FILES:
  # Flag that determines whether or not the workflow will look for the 
  # external model files needed for generating ICs in user-specified
  # directories.
  #
  # EXTRN_MDL_SOURCE_BASEDIR_ICS:
  # Directory in which to look for external model files for generating ICs.
  # If USE_USER_STAGED_EXTRN_FILES is set to true, the workflow looks in 
  # this directory (specifically, in a subdirectory under this directory 
  # named "YYYYMMDDHH" consisting of the starting date and cycle hour of 
  # the forecast, where YYYY is the 4-digit year, MM the 2-digit month, DD 
  # the 2-digit day of the month, and HH the 2-digit hour of the day) for 
  # the external model files specified by the array EXTRN_MDL_FILES_ICS 
  # (these files will be used to generate the ICs on the native FV3-LAM 
  # grid).  This variable is not used if USE_USER_STAGED_EXTRN_FILES is 
  # set to false.
  # 
  # EXTRN_MDL_FILES_ICS:
  # Array containing templates of the names of the files to search for in
  # the directory specified by EXTRN_MDL_SOURCE_BASEDIR_ICS.  This
  # variable is not used if USE_USER_STAGED_EXTRN_FILES is set to false.
  # A single template should be used for each model file type that is
  # meant to be used. You may use any of the Python-style templates
  # allowed in the ush/retrieve_data.py script. To see the full list of
  # supported templates, run that script with a -h option. Here is an example of
  # setting FV3GFS nemsio input files:
  #   EXTRN_MDL_FILES_ICS=( gfs.t{hh}z.atmf{fcst_hr:03d}.nemsio \
  #   gfs.t{hh}z.sfcf{fcst_hr:03d}.nemsio )
  # Or for FV3GFS grib files:
  #   EXTRN_MDL_FILES_ICS=( gfs.t{hh}z.pgrb2.0p25.f{fcst_hr:03d} )
  #
  #-----------------------------------------------------------------------
  #
  USE_USER_STAGED_EXTRN_FILES: false
  EXTRN_MDL_SOURCE_BASEDIR_ICS: ""
  EXTRN_MDL_FILES_ICS: ""

#----------------------------
# EXTRN LBCS config parameters
#-----------------------------
task_get_extrn_lbcs:
  TN_GET_EXTRN_LBCS: "get_extrn_lbcs"
  NNODES_GET_EXTRN_LBCS: 1
  PPN_GET_EXTRN_LBCS: 1
  MEM_GET_EXTRN_LBCS: 2G
  WTIME_GET_EXTRN_LBCS: 00:45:00
  MAXTRIES_GET_EXTRN_LBCS: 1
  #
  #-----------------------------------------------------------------------
  #
  # EXTRN_MDL_NAME_LBCS:
  #`The name of the external model that will provide fields from which 
  # lateral boundary condition (LBC) files (except for the 0-th hour LBC 
  # file) will be generated for input into the forecast model.
  #
  # LBC_SPEC_INTVL_HRS:
  # The interval (in integer hours) with which LBC files will be generated.
  # We will refer to this as the boundary update interval.  Note that the
  # model specified in EXTRN_MDL_NAME_LBCS must have data available at a
  # frequency greater than or equal to that implied by LBC_SPEC_INTVL_HRS.
  # For example, if LBC_SPEC_INTVL_HRS is set to 6, then the model must have
  # data availble at least every 6 hours.  It is up to the user to ensure 
  # that this is the case.
  #
  # EXTRN_MDL_LBCS_OFFSET_HRS:
  # Users may wish to use lateral boundary conditions from a forecast that
  # was started earlier than the initial time for the FV3 forecast
  # configured here. This variable sets the number of hours earlier
  # the external model started than when the FV3 forecast configured here
  # should start. For example, the forecast should use lateral boundary
  # conditions from the GFS started 6 hours earlier, then
  # EXTRN_MDL_LBCS_OFFSET_HRS=6. Defaults to 0 except for RAP, which
  # uses a 3 hour offset.
  #
  # FV3GFS_FILE_FMT_LBCS:
  # If using the FV3GFS model as the source of the LBCs (i.e. if 
  # EXTRN_MDL_NAME_LBCS is set to "FV3GFS"), this variable specifies the 
  # format of the model files to use when generating the LBCs.
  #
  #-----------------------------------------------------------------------
  #
  EXTRN_MDL_NAME_LBCS: "FV3GFS"
  LBC_SPEC_INTVL_HRS: 6
  EXTRN_MDL_LBCS_OFFSET_HRS: '{{ 3 if EXTRN_MDL_NAME_LBCS == "RAP" else 0 }}'
  FV3GFS_FILE_FMT_LBCS: "nemsio"
  #-----------------------------------------------------------------------
  #
  # EXTRN_MDL_SYSBASEDIR_LBCS:
  # Same as EXTRN_MDL_SYSBASEDIR_ICS but for LBCs.
  #
  # Note that this must be defined as a null string here so that if it is 
  # specified by the user in the experiment configuration file, it remains 
  # set to those values, and if not, it gets set to machine-dependent 
  # values.
  #
  #-----------------------------------------------------------------------
  #
  EXTRN_MDL_SYSBASEDIR_LBCS: ''
  #
  #-----------------------------------------------------------------------
  #
  # User-staged external model directories and files.  Definitions:
  #
  # USE_USER_STAGED_EXTRN_FILES:
  # Analogous to USE_USER_STAGED_EXTRN_FILES in ICS but for LBCs
  #
  # EXTRN_MDL_SOURCE_BASEDIR_LBCS:
  # Analogous to EXTRN_MDL_SOURCE_BASEDIR_ICS but for LBCs instead of ICs.
  #
  # EXTRN_MDL_FILES_LBCS:
  # Analogous to EXTRN_MDL_FILES_ICS but for LBCs instead of ICs.
  #
  #-----------------------------------------------------------------------
  #
  USE_USER_STAGED_EXTRN_FILES: false
  EXTRN_MDL_SOURCE_BASEDIR_LBCS: ""
  EXTRN_MDL_FILES_LBCS: ""

#----------------------------
# MAKE ICS config parameters
#-----------------------------
task_make_ics:
  TN_MAKE_ICS: "make_ics"
  NNODES_MAKE_ICS: 4
  PPN_MAKE_ICS: 12
  WTIME_MAKE_ICS: 00:30:00
  MAXTRIES_MAKE_ICS: 1
  KMP_AFFINITY_MAKE_ICS: "scatter"
  OMP_NUM_THREADS_MAKE_ICS: 1
  OMP_STACKSIZE_MAKE_ICS: "1024m"
  #
  #-----------------------------------------------------------------------
  #
  # USE_FVCOM:
  # Flag set to update surface conditions in FV3-LAM with fields generated
  # from the Finite Volume Community Ocean Model (FVCOM). This will
  # replace lake/sea surface temperature, ice surface temperature, and ice
  # placement. FVCOM data must already be interpolated to the desired
  # FV3-LAM grid. This flag will be used in make_ics to modify sfc_data.nc
  # after chgres_cube is run by running the routine process_FVCOM.exe
  #
  # FVCOM_WCSTART:
  # Define if this is a "warm" start or a "cold" start. Setting this to 
  # "warm" will read in sfc_data.nc generated in a RESTART directory.
  # Setting this to "cold" will read in the sfc_data.nc generated from 
  # chgres_cube in the make_ics portion of the workflow.
  #
  # FVCOM_DIR:
  # User defined directory where FVCOM data already interpolated to FV3-LAM
  # grid is located. File name in this path should be "fvcom.nc" to allow
  #
  # FVCOM_FILE:
  # Name of file located in FVCOM_DIR that has FVCOM data interpolated to 
  # FV3-LAM grid. This file will be copied later to a new location and name
  # changed to fvcom.nc
  #
  #------------------------------------------------------------------------
  #
  USE_FVCOM: false
  FVCOM_WCSTART: "cold"
  FVCOM_DIR: ""
  FVCOM_FILE: "fvcom.nc"

#----------------------------
# MAKE LBCS config parameters
#-----------------------------
task_make_lbcs:
  TN_MAKE_LBCS: "make_lbcs"
  NNODES_MAKE_LBCS: 4
  PPN_MAKE_LBCS: 12
  WTIME_MAKE_LBCS: 00:30:00
  MAXTRIES_MAKE_LBCS: 1
  KMP_AFFINITY_MAKE_LBCS: "scatter"
  OMP_NUM_THREADS_MAKE_LBCS: 1
  OMP_STACKSIZE_MAKE_LBCS: "1024m"

#----------------------------
# FORECAST config parameters
#-----------------------------
task_run_fcst:
  TN_RUN_FCST: "run_fcst"
  NNODES_RUN_FCST: '{{ (PE_MEMBER01 + PPN_RUN_FCST - 1) // PPN_RUN_FCST }}'
  PPN_RUN_FCST: '{{ platform.NCORES_PER_NODE // OMP_NUM_THREADS_RUN_FCST }}'
  WTIME_RUN_FCST: 04:30:00
  MAXTRIES_RUN_FCST: 1
  FV3_EXEC_FP: '{{ [user.EXECdir, workflow.FV3_EXEC_FN]|path_join }}'
  #
  #-----------------------------------------------------------------------
  #
  # KMP_AFFINITY_*:
  # From Intel: "The Intel® runtime library has the ability to bind OpenMP
  # threads to physical processing units. The interface is controlled using
  # the KMP_AFFINITY environment variable. Depending on the system (machine)
  # topology, application, and operating system, thread affinity can have a
  # dramatic effect on the application speed. 
  #
  # Thread affinity restricts execution of certain threads (virtual execution
  # units) to a subset of the physical processing units in a multiprocessor 
  # computer. Depending upon the topology of the machine, thread affinity can
  # have a dramatic effect on the execution speed of a program."
  #
  # For more information, see the following link:
  # https://software.intel.com/content/www/us/en/develop/documentation/cpp-
  # compiler-developer-guide-and-reference/top/optimization-and-programming-
  # guide/openmp-support/openmp-library-support/thread-affinity-interface-
  # linux-and-windows.html
  # 
  # OMP_NUM_THREADS_*:
  # The number of OpenMP threads to use for parallel regions.
  # 
  # OMP_STACKSIZE_*:
  # Controls the size of the stack for threads created by the OpenMP 
  # implementation.
  #
  # Note that settings for the make_grid and make_orog tasks are not 
  # included below as they do not use parallelized code.
  #
  #-----------------------------------------------------------------------
  #
  KMP_AFFINITY_RUN_FCST: "scatter"
  OMP_NUM_THREADS_RUN_FCST: 1    # ATM_omp_num_threads in nems.configure
  OMP_STACKSIZE_RUN_FCST: "512m"
  #
  #-----------------------------------------------------------------------
  #
  # Set model_configure parameters.  Definitions:
  #
  # DT_ATMOS:
  # The main forecast model integration time step.  As described in the 
  # forecast model documentation, "It corresponds to the frequency with 
  # which the top level routine in the dynamics is called as well as the 
  # frequency with which the physics is called."
  #
  # FHROT:
  # Forecast hour at restart
  #
  # RESTART_INTERVAL:
  # frequency of the output restart files (unit:hour). 
  # Default=0: restart files are produced at the end of a forecast run
  # For example, i) RESTART_INTERVAL: 1 -1 => restart files are produced 
  # every hour with the prefix "YYYYMMDD.HHmmSS." in the RESTART directory
  # ii) RESTART_INTERVAL: 1 2 5 => restart files are produced only when 
  # fh = 1, 2, and 5.
  #
  # WRITE_DOPOST:
  # Flag that determines whether or not to use the inline post feature 
  # [i.e. calling the Unified Post Processor (UPP) from within the weather 
  # model].  If this is set to true, the TN_RUN_POST task is deactivated 
  # (i.e. RUN_TASK_RUN_POST is set to false) to avoid unnecessary 
  # computations.
  #
  #-----------------------------------------------------------------------
  #
  DT_ATMOS: ""
  FHROT: 0
  RESTART_INTERVAL: 0
  WRITE_DOPOST: false
  #
  #-----------------------------------------------------------------------
  #
  # Set computational parameters for the forecast.  Definitions:
  #
  # LAYOUT_X, LAYOUT_Y:
  # The number of MPI tasks (processes) to use in the two horizontal 
  # directions (x and y) of the regional grid when running the forecast 
  # model.
  #
  # BLOCKSIZE:
  # The amount of data that is passed into the cache at a time.
  #
  # Here, we set these parameters to null strings.  This is so that, for 
  # any one of these parameters:
  #
  # 1) If the experiment is using a predefined grid, then if the user 
  #    sets the parameter in the user-specified experiment configuration 
  #    file (EXPT_CONFIG_FN), that value will be used in the forecast(s).
  #    Otherwise, the default value of the parameter for that predefined 
  #    grid will be used.
  #
  # 2) If the experiment is not using a predefined grid (i.e. it is using
  #    a custom grid whose parameters are specified in the experiment 
  #    configuration file), then the user must specify a value for the 
  #    parameter in that configuration file.  Otherwise, the parameter 
  #    will remain set to a null string, and the experiment generation 
  #    will fail because the generation scripts check to ensure that all 
  #    the parameters defined in this section are set to non-empty strings
  #    before creating the experiment directory.
  #
  #-----------------------------------------------------------------------
  #
  LAYOUT_X: '{{ LAYOUT_X }}'
  LAYOUT_Y: '{{ LAYOUT_Y }}'
  BLOCKSIZE: '{{ BLOCKSIZE }}'
  #
  #-----------------------------------------------------------------------
  #
  # Set write-component (quilting) parameters.  Definitions:
  #
  # QUILTING:
  # Flag that determines whether or not to use the write component for 
  # writing output files to disk. The regional grid requires the use of 
  # the write component, so users should not change the default value. 
  #
  # PRINT_ESMF:
  # Flag for whether or not to output extra (debugging) information from
  # ESMF routines. Must be true or false. Note that the write
  # component uses ESMF library routines to interpolate from the native
  # forecast model grid to the user-specified output grid (which is defined 
  # in the model configuration file "model_configure" in the forecast's  
  # run directory).
  # 
  # WRTCMP_write_groups:
  # The number of write groups (i.e. groups of MPI tasks) to use in the
  # write component.
  #
  # WRTCMP_write_tasks_per_group:
  # The number of MPI tasks to allocate for each write group.
  #
  # WRTCMP_output_grid:
  # Sets the type (coordinate system) of the write component grid. The 
  # default empty string forces the user to set a valid value for 
  # WRTCMP_output_grid in config.yaml if specifying a *custom* grid. When 
  # creating an experiment with a user-defined grid, this parameter must 
  # be specified or the experiment will fail. 
  #
  # WRTCMP_cen_lon:
  # Longitude (in degrees) of the center of the write component grid. Can 
  # usually be set to the corresponding value from the native grid.
  #
  # WRTCMP_cen_lat:
  # Latitude (in degrees) of the center of the write component grid. Can 
  # usually be set to the corresponding value from the native grid.
  # WRTCMP_lon_lwr_left:
  # Longitude (in degrees) of the center of the lower-left (southwest) 
  # cell on the write component grid. If using the "rotated_latlon" 
  # coordinate system, this is expressed in terms of the rotated longitude. 
  # Must be set manually when running an experiment with a user-defined grid.
  #
  # WRTCMP_lat_lwr_left:
  # Latitude (in degrees) of the center of the lower-left (southwest) cell 
  # on the write component grid. If using the "rotated_latlon" coordinate 
  # system, this is expressed in terms of the rotated latitude. Must be set 
  # manually when running an experiment with a user-defined grid.
  # 
  # -----------------------------------------------------------------------
  # 
  # WRTCMP_lon_upr_rght:
  # Longitude (in degrees) of the center of the upper-right (northeast) cell 
  # on the write component grid (expressed in terms of the rotated longitude).
  #
  # WRTCMP_lat_upr_rght:
  # Latitude (in degrees) of the center of the upper-right (northeast) cell 
  # on the write component grid (expressed in terms of the rotated latitude).
  #
  # WRTCMP_dlon:
  # Size (in degrees) of a grid cell on the write component grid (expressed 
  # in terms of the rotated longitude).
  #
  # WRTCMP_dlat:
  # Size (in degrees) of a grid cell on the write component grid (expressed 
  # in terms of the rotated latitude).
  #
  # -----------------------------------------------------------------------
  # 
  # WRTCMP_stdlat1:
  # First standard latitude (in degrees) in definition of Lambert conformal 
  # projection.
  #
  # WRTCMP_stdlat2:
  # Second standard latitude (in degrees) in definition of Lambert conformal 
  # projection.
  #
  # WRTCMP_nx:
  # Number of grid points in the x-coordinate of the Lambert conformal 
  # projection.
  #
  # WRTCMP_ny:
  # Number of grid points in the y-coordinate of the Lambert conformal 
  # projection.
  #
  # WRTCMP_dx:
  # Grid cell size (in meters) along the x-axis of the Lambert conformal 
  # projection.
  #
  # WRTCMP_dy:
  # Grid cell size (in meters) along the y-axis of the Lambert conformal 
  # projection. 
  #
  #-----------------------------------------------------------------------
  #
  QUILTING: true
  PRINT_ESMF: false

  PE_MEMBER01: '{{ LAYOUT_Y * LAYOUT_X + WRTCMP_write_groups * WRTCMP_write_tasks_per_group if QUILTING else LAYOUT_Y * LAYOUT_X}}'
  
  WRTCMP_write_groups: ""
  WRTCMP_write_tasks_per_group: ""
  
  WRTCMP_output_grid: "''"
  WRTCMP_cen_lon: ""
  WRTCMP_cen_lat: ""
  WRTCMP_lon_lwr_left: ""
  WRTCMP_lat_lwr_left: ""
  #
  # The following are used only for the case of WRTCMP_output_grid set to
  # "'rotated_latlon'".
  #
  WRTCMP_lon_upr_rght: ""
  WRTCMP_lat_upr_rght: ""
  WRTCMP_dlon: ""
  WRTCMP_dlat: ""
  #
  # The following are used only for the case of WRTCMP_output_grid set to
  # "'lambert_conformal'".
  #
  WRTCMP_stdlat1: ""
  WRTCMP_stdlat2: ""
  WRTCMP_nx: ""
  WRTCMP_ny: ""
  WRTCMP_dx: ""
  WRTCMP_dy: ""
  #
  #-----------------------------------------------------------------------
  #
  # Flag that determines whether MERRA2 aerosol climatology data and
  # lookup tables for optics properties are obtained
  #
  #-----------------------------------------------------------------------
  #
  USE_MERRA_CLIMO: '{{ workflow.CCPP_PHYS_SUITE == "FV3_GFS_v15_thompson_mynn_lam3km" or workflow.CCPP_PHYS_SUITE == "FV3_GFS_v17_p8" }}'
  #
  #-----------------------------------------------------------------------
  #
  # DO_FCST_RESTART:
  # Flag turning on/off restart capability of forecast task
  #
  #-----------------------------------------------------------------------
  #
  DO_FCST_RESTART: false

#----------------------------
# POST config parameters
#-----------------------------
task_run_post:
  TN_RUN_POST: "run_post"
  NNODES_RUN_POST: 2
  PPN_RUN_POST: 24
  WTIME_RUN_POST: 00:15:00
  MAXTRIES_RUN_POST: 2
  KMP_AFFINITY_RUN_POST: "scatter"
  OMP_NUM_THREADS_RUN_POST: 1
  OMP_STACKSIZE_RUN_POST: "1024m"
  #
  #-----------------------------------------------------------------------
  #
  # Set parameters associated with subhourly forecast model output and 
  # post-processing.
  #
  # SUB_HOURLY_POST:
  # Flag that indicates whether the forecast model will generate output 
  # files on a sub-hourly time interval (e.g. 10 minutes, 15 minutes, etc).
  # This will also cause the post-processor to process these sub-hourly
  # files.  If ths is set to true, then DT_SUBHOURLY_POST_MNTS should be 
  # set to a value between "00" and "59".
  #
  # DT_SUB_HOURLY_POST_MNTS:
  # Time interval in minutes between the forecast model output files.  If 
  # SUB_HOURLY_POST is set to true, this needs to be set to a two-digit 
  # integer between "01" and "59".  This is not used if SUB_HOURLY_POST is
  # not set to true.  Note that if SUB_HOURLY_POST is set to true but
  # DT_SUB_HOURLY_POST_MNTS is set to "00", SUB_HOURLY_POST will get reset
  # to false in the experiment generation scripts (there will be an 
  # informational message in the log file to emphasize this).
  #
  #-----------------------------------------------------------------------
  #
  SUB_HOURLY_POST: false
  DT_SUBHOURLY_POST_MNTS: 0
  #
  #-----------------------------------------------------------------------
  #
  # Set parameters for customizing the post-processor (UPP).  Definitions:
  #
  # USE_CUSTOM_POST_CONFIG_FILE:
  # Flag that determines whether a user-provided custom configuration file
  # should be used for post-processing the model data. If this is set to
  # true, then the workflow will use the custom post-processing (UPP) 
  # configuration file specified in CUSTOM_POST_CONFIG_FP. Otherwise, a 
  # default configuration file provided in the UPP repository will be 
  # used.
  #
  # CUSTOM_POST_CONFIG_FP:
  # The full path to the custom post flat file, including filename, to be 
  # used for post-processing. This is only used if CUSTOM_POST_CONFIG_FILE
  # is set to true.
  #
  # TESTBED_FIELDS_FN
  # The file which lists grib2 fields to be extracted for testbed files
  # Empty string means no need to generate testbed files
  #
  # POST_OUTPUT_DOMAIN_NAME:
  # Domain name (in lowercase) used in constructing the names of the output 
  # files generated by UPP [which is called either by running the TN_RUN_POST 
  # task or by activating the inline post feature (WRITE_DOPOST set to true)].  
  # The post output files are named as follows:
  # 
  #   $NET.tHHz.[var_name].f###.${POST_OUTPUT_DOMAIN_NAME}.grib2
  # 
  # If using a custom grid, POST_OUTPUT_DOMAIN_NAME must be specified by 
  # the user.  If using a predefined grid, POST_OUTPUT_DOMAIN_NAME defaults
  # to PREDEF_GRID_NAME.  Note that this variable is first changed to lower
  # case before being used to construct the file names.
  #
  #-----------------------------------------------------------------------
  #
  USE_CUSTOM_POST_CONFIG_FILE: false
  CUSTOM_POST_CONFIG_FP: ""
  POST_OUTPUT_DOMAIN_NAME: '{{ workflow.PREDEF_GRID_NAME }}'
  TESTBED_FIELDS_FN: ""

#----------------------------
# RUN PRDGEN config parameters
#-----------------------------
task_run_prdgen:
  TN_RUN_PRDGEN: "run_prdgen"
  NNODES_RUN_PRDGEN: 1
  PPN_RUN_PRDGEN: 22
  WTIME_RUN_PRDGEN: 00:30:00
  MAXTRIES_RUN_PRDGEN: 1
  KMP_AFFINITY_RUN_PRDGEN: "scatter"
  OMP_NUM_THREADS_RUN_PRDGEN: 1
  OMP_STACKSIZE_RUN_PRDGEN: "1024m"
  #-----------------------------------------------------------------------
  #
  # Flag that determines whether to use CFP to run the product generation
  # job in parallel.  This should be used with the RRFS_NA_3km grid.
  #
  #-----------------------------------------------------------------------
  DO_PARALLEL_PRDGEN: false
  #
  #
  #-----------------------------------------------------------------------
  #
  # Set additional output grids for wgrib2 remapping, if any
  # Space-separated list of strings, e.g., ( "130" "242" "clue" )
  # Default is no additional grids
  #
  # Current options as of 23 Apr 2021:
  #  "130"   (CONUS 13.5 km)
  #  "200"   (Puerto Rico 16 km)
  #  "221"   (North America 32 km)
  #  "242"   (Alaska 11.25 km)
  #  "243"   (Pacific 0.4-deg)
  #  "clue"  (NSSL/SPC 3-km CLUE grid for 2020/2021)
  #  "hrrr"  (HRRR 3-km CONUS grid)
  #  "hrrre" (HRRRE 3-km CONUS grid)
  #  "rrfsak" (RRFS 3-km Alaska grid)
  #  "hrrrak" (HRRR 3-km Alaska grid)
  #
  #-----------------------------------------------------------------------
  #
  ADDNL_OUTPUT_GRIDS: []

#----------------------------
# PLOT_ALLVARS config parameters
#-----------------------------
task_plot_allvars:
  TN_PLOT_ALLVARS: "plot_allvars"
  NNODES_PLOT_ALLVARS: 1
  PPN_PLOT_ALLVARS: 24
  WTIME_PLOT_ALLVARS: 01:00:00
  MAXTRIES_PLOT_ALLVARS: 1
  #-------------------------------------------------------------------------
  # Reference experiment's COMOUT directory. This is where the GRIB2 files 
  # from postprocessing are located. Make this a template to compare
  # multiple cycle and dates. COMOUT_REF should end with:
  #    nco mode: $PDY/$cyc
  #    community mode: $PDY$cyc/postprd
  # We don't do this inside the code, so that we can compare nco vs com runs.
  #-------------------------------------------------------------------------
  COMOUT_REF: ""
  #------------------------------
  # Plot fcts start and increment
  #------------------------------
  PLOT_FCST_START: 0
  PLOT_FCST_INC: 3
  #-----------------------------------
  # By default the end is FCST_LEN_HRS
  #-----------------------------------
  PLOT_FCST_END: ""
  #------------------------------------------------------------------------------
  # Domains to plot. Currently supported are either "conus" or "regional" or both
  #-------------------------------------------------------------------------------
  PLOT_DOMAINS: ["conus"]

#----------------------------
# GET OBS CCPA config parameters
#-----------------------------
task_get_obs_ccpa:
  TN_GET_OBS_CCPA: "get_obs_ccpa"
  NNODES_GET_OBS_CCPA: 1
  PPN_GET_OBS_CCPA: 1
  MEM_GET_OBS_CCPA: 2G
  WTIME_GET_OBS_CCPA: 00:45:00
  MAXTRIES_GET_OBS_CCPA: 1

#----------------------------
# GET OBS MRMS config parameters
#-----------------------------
task_get_obs_mrms:
  TN_GET_OBS_MRMS: "get_obs_mrms"
  NNODES_GET_OBS_MRMS: 1
  PPN_GET_OBS_MRMS: 1
  MEM_GET_OBS_MRMS: 2G
  WTIME_GET_OBS_MRMS: 00:45:00
  MAXTRIES_GET_OBS_MRMS: 1

#----------------------------
# GET OBS NDAS config parameters
#-----------------------------
task_get_obs_ndas:
  TN_GET_OBS_NDAS: "get_obs_ndas"
  NNODES_GET_OBS_NDAS: 1
  PPN_GET_OBS_NDAS: 1
  MEM_GET_OBS_NDAS: 2G
  WTIME_GET_OBS_NDAS: 02:00:00
  MAXTRIES_GET_OBS_NDAS: 1

#----------------------------
# tn_run_met_pb2nc_obs config parameters
#-----------------------------
task_tn_run_met_pb2nc_obs:
  TN_RUN_MET_PB2NC_OBS: "run_MET_Pb2nc_obs"
  NNODES_RUN_MET_PB2NC_OBS: 1
  PPN_RUN_MET_PB2NC_OBS: 1
  MEM_RUN_MET_PB2NC_OBS: 2G
  WTIME_RUN_MET_PB2NC_OBS: 00:30:00
  MAXTRIES_RUN_MET_PB2NC_OBS: 2

#----------------------------
# tn_run_met_pcpcombine config parameters
#-----------------------------
task_tn_run_met_pcpcombine:
  TN_RUN_MET_PCPCOMBINE: "run_MET_PcpCombine"
#
  NNODES_RUN_MET_PCPCOMBINE_OBS: 1
  PPN_RUN_MET_PCPCOMBINE_OBS: 1
  MEM_RUN_MET_PCPCOMBINE_OBS: 2G
  WTIME_RUN_MET_PCPCOMBINE_OBS: 00:30:00
  MAXTRIES_RUN_MET_PCPCOMBINE_OBS: 2
#
  NNODES_RUN_MET_PCPCOMBINE_FCST: 1
  PPN_RUN_MET_PCPCOMBINE_FCST: 1
  MEM_RUN_MET_PCPCOMBINE_FCST: 2G
  WTIME_RUN_MET_PCPCOMBINE_FCST: 00:30:00
  MAXTRIES_RUN_MET_PCPCOMBINE_FCST: 2
  
#----------------------------
# run_met_gridstat_vx_apcp01h config parameters
#-----------------------------
task_run_met_gridstat_vx_apcp01h:
  TN_RUN_MET_GRIDSTAT_VX_APCP01H: "run_MET_GridStat_vx_APCP01h"
  NNODES_RUN_MET_GRIDSTAT_VX_APCP01H: 1
  PPN_RUN_MET_GRIDSTAT_VX_APCP01H: 1
  MEM_RUN_MET_GRIDSTAT_VX_APCP01H: 2G
  WTIME_RUN_MET_GRIDSTAT_VX_APCP01H: 02:00:00
  MAXTRIES_RUN_MET_GRIDSTAT_VX_APCP01H: 2

#----------------------------
# run_met_gridstat_vx_apcp03h config parameters
#-----------------------------
task_run_met_gridstat_vx_apcp03h:
  TN_RUN_MET_GRIDSTAT_VX_APCP03H: "run_MET_GridStat_vx_APCP03h"
  NNODES_RUN_MET_GRIDSTAT_VX_APCP03H: 1
  PPN_RUN_MET_GRIDSTAT_VX_APCP03H: 1
  MEM_RUN_MET_GRIDSTAT_VX_APCP03H: 2G
  WTIME_RUN_MET_GRIDSTAT_VX_APCP03H: 02:00:00
  MAXTRIES_RUN_MET_GRIDSTAT_VX_APCP03H: 2

#----------------------------
# run_met_gridstat_vx_apcp06h config parameters
#-----------------------------
task_run_met_gridstat_vx_apcp06h:
  TN_RUN_MET_GRIDSTAT_VX_APCP06H: "run_MET_GridStat_vx_APCP06h"
  NNODES_RUN_MET_GRIDSTAT_VX_APCP06H: 1
  PPN_RUN_MET_GRIDSTAT_VX_APCP06H: 1
  MEM_RUN_MET_GRIDSTAT_VX_APCP06H: 2G
  WTIME_RUN_MET_GRIDSTAT_VX_APCP06H: 02:00:00
  MAXTRIES_RUN_MET_GRIDSTAT_VX_APCP06H: 2

#----------------------------
# run_met_gridstat_vx_apcp24h config parameters
#-----------------------------
task_run_met_gridstat_vx_apcp24h:
  TN_RUN_MET_GRIDSTAT_VX_APCP24H: "run_MET_GridStat_vx_APCP24h"
  NNODES_RUN_MET_GRIDSTAT_VX_APCP24H: 1
  PPN_RUN_MET_GRIDSTAT_VX_APCP24H: 1
  MEM_RUN_MET_GRIDSTAT_VX_APCP24H: 2G
  WTIME_RUN_MET_GRIDSTAT_VX_APCP24H: 02:00:00
  MAXTRIES_RUN_MET_GRIDSTAT_VX_APCP24H: 2

#----------------------------
# run_met_gridstat_vx_refc config parameters
#-----------------------------
task_run_met_gridstat_vx_refc:
  TN_RUN_MET_GRIDSTAT_VX_REFC: "run_MET_GridStat_vx_REFC"
  NNODES_RUN_MET_GRIDSTAT_VX_REFC: 1
  PPN_RUN_MET_GRIDSTAT_VX_REFC: 1
  MEM_RUN_MET_GRIDSTAT_VX_REFC: 2G
  WTIME_RUN_MET_GRIDSTAT_VX_REFC: 02:00:00
  MAXTRIES_RUN_MET_GRIDSTAT_VX_REFC: 2

#----------------------------
# run_met_gridstat_vx_retop config parameters
#-----------------------------
task_run_met_gridstat_vx_retop:
  TN_RUN_MET_GRIDSTAT_VX_RETOP: "run_MET_GridStat_vx_RETOP"
  NNODES_RUN_MET_GRIDSTAT_VX_RETOP: 1
  PPN_RUN_MET_GRIDSTAT_VX_RETOP: 1
  MEM_RUN_MET_GRIDSTAT_VX_RETOP: 2G
  WTIME_RUN_MET_GRIDSTAT_VX_RETOP: 02:00:00
  MAXTRIES_RUN_MET_GRIDSTAT_VX_RETOP: 2

#----------------------------
# run_met_pointstat_vx_sfc config parameters
#-----------------------------
task_run_met_pointstat_vx_sfc:
  TN_RUN_MET_POINTSTAT_VX_SFC: "run_MET_PointStat_vx_SFC"
  NNODES_RUN_MET_POINTSTAT_VX_SFC: 1
  PPN_RUN_MET_POINTSTAT_VX_SFC: 1
  MEM_RUN_MET_POINTSTAT_VX_SFC: 2G
  WTIME_RUN_MET_POINTSTAT_VX_SFC: 01:00:00
  MAXTRIES_RUN_MET_POINTSTAT_VX_SFC: 2

#----------------------------
# run_met_pointstat_vx_upa config parameters
#-----------------------------
task_run_met_pointstat_vx_upa:
  TN_RUN_MET_POINTSTAT_VX_UPA: "run_MET_PointStat_vx_UPA"
  NNODES_RUN_MET_POINTSTAT_VX_UPA: 1
  PPN_RUN_MET_POINTSTAT_VX_UPA: 1
  MEM_RUN_MET_POINTSTAT_VX_UPA: 2G
  WTIME_RUN_MET_POINTSTAT_VX_UPA: 01:00:00
  MAXTRIES_RUN_MET_POINTSTAT_VX_UPA: 2

#----------------------------
# run_met_ensemblestat_vx_apcp01h config parameters
#-----------------------------
task_run_met_ensemblestat_vx_apcp01h:
  TN_RUN_MET_ENSEMBLESTAT_VX_APCP01H: "run_MET_EnsembleStat_vx_APCP01h"
  NNODES_RUN_MET_ENSEMBLESTAT_VX_APCP01H: 1
  PPN_RUN_MET_ENSEMBLESTAT_VX_APCP01H: 1
  MEM_RUN_MET_ENSEMBLESTAT_VX_APCP01H: 2G
  WTIME_RUN_MET_ENSEMBLESTAT_VX_APCP01H: 01:00:00
  MAXTRIES_RUN_MET_ENSEMBLESTAT_VX_APCP01H: 2

#----------------------------
# run_met_ensemblestat_vx_apcp03h config parameters
#-----------------------------
task_run_met_ensemblestat_vx_apcp03h:
  TN_RUN_MET_ENSEMBLESTAT_VX_APCP03H: "run_MET_EnsembleStat_vx_APCP03h"
  NNODES_RUN_MET_ENSEMBLESTAT_VX_APCP03H: 1
  PPN_RUN_MET_ENSEMBLESTAT_VX_APCP03H: 1
  MEM_RUN_MET_ENSEMBLESTAT_VX_APCP03H: 2G
  WTIME_RUN_MET_ENSEMBLESTAT_VX_APCP03H: 01:00:00
  MAXTRIES_RUN_MET_ENSEMBLESTAT_VX_APCP03H: 2

#----------------------------
# run_met_ensemblestat_vx_apcp06h config parameters
#-----------------------------
task_run_met_ensemblestat_vx_apcp06h:
  TN_RUN_MET_ENSEMBLESTAT_VX_APCP06H: "run_MET_EnsembleStat_vx_APCP06h"
  NNODES_RUN_MET_ENSEMBLESTAT_VX_APCP06H: 1
  PPN_RUN_MET_ENSEMBLESTAT_VX_APCP06H: 1
  MEM_RUN_MET_ENSEMBLESTAT_VX_APCP06H: 2G
  WTIME_RUN_MET_ENSEMBLESTAT_VX_APCP06H: 01:00:00
  MAXTRIES_RUN_MET_ENSEMBLESTAT_VX_APCP06H: 2

#----------------------------
# run_met_ensemblestat_vx_apcp24h config parameters
#-----------------------------
task_run_met_ensemblestat_vx_apcp24h:
  TN_RUN_MET_ENSEMBLESTAT_VX_APCP24H: "run_MET_EnsembleStat_vx_APCP24h"
  NNODES_RUN_MET_ENSEMBLESTAT_VX_APCP24H: 1
  PPN_RUN_MET_ENSEMBLESTAT_VX_APCP24H: 1
  MEM_RUN_MET_ENSEMBLESTAT_VX_APCP24H: 2G
  WTIME_RUN_MET_ENSEMBLESTAT_VX_APCP24H: 01:00:00
  MAXTRIES_RUN_MET_ENSEMBLESTAT_VX_APCP24H: 2

#----------------------------
# run_met_ensemblestat_vx_refc config parameters
#-----------------------------
task_run_met_ensemblestat_vx_refc:
  TN_RUN_MET_ENSEMBLESTAT_VX_REFC: "run_MET_EnsembleStat_vx_REFC"
  NNODES_RUN_MET_ENSEMBLESTAT_VX_REFC: 1
  PPN_RUN_MET_ENSEMBLESTAT_VX_REFC: 1
  MEM_RUN_MET_ENSEMBLESTAT_VX_REFC: 2G
  WTIME_RUN_MET_ENSEMBLESTAT_VX_REFC: 01:00:00
  MAXTRIES_RUN_MET_ENSEMBLESTAT_VX_REFC: 2

#----------------------------
# run_met_ensemblestat_vx_retop config parameters
#-----------------------------
task_run_met_ensemblestat_vx_retop:
  TN_RUN_MET_ENSEMBLESTAT_VX_RETOP: "run_MET_EnsembleStat_vx_RETOP"
  NNODES_RUN_MET_ENSEMBLESTAT_VX_RETOP: 1
  PPN_RUN_MET_ENSEMBLESTAT_VX_RETOP: 1
  MEM_RUN_MET_ENSEMBLESTAT_VX_RETOP: 2G
  WTIME_RUN_MET_ENSEMBLESTAT_VX_RETOP: 01:00:00
  MAXTRIES_RUN_MET_ENSEMBLESTAT_VX_RETOP: 2

#----------------------------
# run_met_ensemblestat_vx_sfc config parameters
#-----------------------------
task_run_met_ensemblestat_vx_sfc:
  TN_RUN_MET_ENSEMBLESTAT_VX_SFC: "run_MET_EnsembleStat_vx_SFC"
  NNODES_RUN_MET_ENSEMBLESTAT_VX_SFC: 1
  PPN_RUN_MET_ENSEMBLESTAT_VX_SFC: 1
  MEM_RUN_MET_ENSEMBLESTAT_VX_SFC: 2G
  WTIME_RUN_MET_ENSEMBLESTAT_VX_SFC: 01:00:00
  MAXTRIES_RUN_MET_ENSEMBLESTAT_VX_SFC: 2

#----------------------------
# run_met_ensemblestat_vx_upa config parameters
#-----------------------------
task_run_met_ensemblestat_vx_upa:
  TN_RUN_MET_ENSEMBLESTAT_VX_UPA: "run_MET_EnsembleStat_vx_UPA"
  NNODES_RUN_MET_ENSEMBLESTAT_VX_UPA: 1
  PPN_RUN_MET_ENSEMBLESTAT_VX_UPA: 1
  MEM_RUN_MET_ENSEMBLESTAT_VX_UPA: 2G
  WTIME_RUN_MET_ENSEMBLESTAT_VX_UPA: 01:00:00
  MAXTRIES_RUN_MET_ENSEMBLESTAT_VX_UPA: 2

#----------------------------
# run_met_gridstat_vx_ensmean_apcp01h config parameters
#-----------------------------
task_run_met_gridstat_vx_ensmean_apcp01h:
  TN_RUN_MET_GRIDSTAT_VX_ENSMEAN_APCP01H: "run_MET_GridStat_vx_ensmean_APCP01h"
  NNODES_RUN_MET_GRIDSTAT_VX_ENSMEAN_APCP01H: 1
  PPN_RUN_MET_GRIDSTAT_VX_ENSMEAN_APCP01H: 1
  MEM_RUN_MET_GRIDSTAT_VX_ENSMEAN_APCP01H: 2G
  WTIME_RUN_MET_GRIDSTAT_VX_ENSMEAN_APCP01H: 01:00:00
  MAXTRIES_RUN_MET_GRIDSTAT_VX_ENSMEAN_APCP01H: 2

#----------------------------
# run_met_gridstat_vx_ensmean_apcp03h config parameters
#-----------------------------
task_run_met_gridstat_vx_ensmean_apcp03h:
  TN_RUN_MET_GRIDSTAT_VX_ENSMEAN_APCP03H: "run_MET_GridStat_vx_ensmean_APCP03h"
  NNODES_RUN_MET_GRIDSTAT_VX_ENSMEAN_APCP03H: 1
  PPN_RUN_MET_GRIDSTAT_VX_ENSMEAN_APCP03H: 1
  MEM_RUN_MET_GRIDSTAT_VX_ENSMEAN_APCP03H: 2G
  WTIME_RUN_MET_GRIDSTAT_VX_ENSMEAN_APCP03H: 01:00:00
  MAXTRIES_RUN_MET_GRIDSTAT_VX_ENSMEAN_APCP03H: 2

#----------------------------
# run_met_gridstat_vx_ensmean_apcp06h config parameters
#-----------------------------
task_run_met_gridstat_vx_ensmean_apcp06h:
  TN_RUN_MET_GRIDSTAT_VX_ENSMEAN_APCP06H: "run_MET_GridStat_vx_ensmean_APCP06h"
  NNODES_RUN_MET_GRIDSTAT_VX_ENSMEAN_APCP06H: 1
  PPN_RUN_MET_GRIDSTAT_VX_ENSMEAN_APCP06H: 1
  MEM_RUN_MET_GRIDSTAT_VX_ENSMEAN_APCP06H: 2G
  WTIME_RUN_MET_GRIDSTAT_VX_ENSMEAN_APCP06H: 01:00:00
  MAXTRIES_RUN_MET_GRIDSTAT_VX_ENSMEAN_APCP06H: 2

#----------------------------
# run_met_gridstat_vx_ensmean_apcp24h config parameters
#-----------------------------
task_run_met_gridstat_vx_ensmean_apcp24h:
  TN_RUN_MET_GRIDSTAT_VX_ENSMEAN_APCP24H: "run_MET_GridStat_vx_ensmean_APCP24h"
  NNODES_RUN_MET_GRIDSTAT_VX_ENSMEAN_APCP24H: 1
  PPN_RUN_MET_GRIDSTAT_VX_ENSMEAN_APCP24H: 1
  MEM_RUN_MET_GRIDSTAT_VX_ENSMEAN_APCP24H: 2G
  WTIME_RUN_MET_GRIDSTAT_VX_ENSMEAN_APCP24H: 01:00:00
  MAXTRIES_RUN_MET_GRIDSTAT_VX_ENSMEAN_APCP24H: 2

#----------------------------
# run_met_pointstat_vx_ensmean_sfc config parameters
#-----------------------------
task_run_met_pointstat_vx_ensmean_sfc:
  TN_RUN_MET_POINTSTAT_VX_ENSMEAN_SFC: "run_MET_PointStat_vx_ensmean_SFC"
  NNODES_RUN_MET_POINTSTAT_VX_ENSMEAN_SFC: 1
  PPN_RUN_MET_POINTSTAT_VX_ENSMEAN_SFC: 1
  MEM_RUN_MET_POINTSTAT_VX_ENSMEAN_SFC: 2G
  WTIME_RUN_MET_POINTSTAT_VX_ENSMEAN_SFC: 01:00:00
  MAXTRIES_RUN_MET_POINTSTAT_VX_ENSMEAN_SFC: 2

#----------------------------
# run_met_pointstat_vx_ensmean_upa config parameters
#-----------------------------
task_run_met_pointstat_vx_ensmean_upa:
  TN_RUN_MET_POINTSTAT_VX_ENSMEAN_UPA: "run_MET_PointStat_vx_ensmean_UPA"
  NNODES_RUN_MET_POINTSTAT_VX_ENSMEAN_UPA: 1
  PPN_RUN_MET_POINTSTAT_VX_ENSMEAN_UPA: 1
  MEM_RUN_MET_POINTSTAT_VX_ENSMEAN_UPA: 2G
  WTIME_RUN_MET_POINTSTAT_VX_ENSMEAN_UPA: 01:00:00
  MAXTRIES_RUN_MET_POINTSTAT_VX_ENSMEAN_UPA: 2

#----------------------------
# run_met_gridstat_vx_ensprob_apcp01h config parameters
#-----------------------------
task_run_met_gridstat_vx_ensprob_apcp01h:
  TN_RUN_MET_GRIDSTAT_VX_ENSPROB_APCP01H: "run_MET_GridStat_vx_ensprob_APCP01h"
  NNODES_RUN_MET_GRIDSTAT_VX_ENSPROB_APCP01H: 1
  PPN_RUN_MET_GRIDSTAT_VX_ENSPROB_APCP01H: 1
  MEM_RUN_MET_GRIDSTAT_VX_ENSPROB_APCP01H: 2G
  WTIME_RUN_MET_GRIDSTAT_VX_ENSPROB_APCP01H: 01:00:00
  MAXTRIES_RUN_MET_GRIDSTAT_VX_ENSPROB_APCP01H: 2

#----------------------------
# run_met_gridstat_vx_ensprob_apcp03h config parameters
#-----------------------------
task_run_met_gridstat_vx_ensprob_apcp03h:
  TN_RUN_MET_GRIDSTAT_VX_ENSPROB_APCP03H: "run_MET_GridStat_vx_ensprob_APCP03h"
  NNODES_RUN_MET_GRIDSTAT_VX_ENSPROB_APCP03H: 1
  PPN_RUN_MET_GRIDSTAT_VX_ENSPROB_APCP03H: 1
  MEM_RUN_MET_GRIDSTAT_VX_ENSPROB_APCP03H: 2G
  WTIME_RUN_MET_GRIDSTAT_VX_ENSPROB_APCP03H: 01:00:00
  MAXTRIES_RUN_MET_GRIDSTAT_VX_ENSPROB_APCP03H: 2

#----------------------------
# run_met_gridstat_vx_ensprob_apcp06h config parameters
#-----------------------------
task_run_met_gridstat_vx_ensprob_apcp06h:
  TN_RUN_MET_GRIDSTAT_VX_ENSPROB_APCP06H: "run_MET_GridStat_vx_ensprob_APCP06h"
  NNODES_RUN_MET_GRIDSTAT_VX_ENSPROB_APCP06H: 1
  PPN_RUN_MET_GRIDSTAT_VX_ENSPROB_APCP06H: 1
  MEM_RUN_MET_GRIDSTAT_VX_ENSPROB_APCP06H: 2G
  WTIME_RUN_MET_GRIDSTAT_VX_ENSPROB_APCP06H: 01:00:00
  MAXTRIES_RUN_MET_GRIDSTAT_VX_ENSPROB_APCP06H: 2

#----------------------------
# run_met_gridstat_vx_ensprob_apcp24h config parameters
#-----------------------------
task_run_met_gridstat_vx_ensprob_apcp24h:
  TN_RUN_MET_GRIDSTAT_VX_ENSPROB_APCP24H: "run_MET_GridStat_vx_ensprob_APCP24h"
  NNODES_RUN_MET_GRIDSTAT_VX_ENSPROB_APCP24H: 1
  PPN_RUN_MET_GRIDSTAT_VX_ENSPROB_APCP24H: 1
  MEM_RUN_MET_GRIDSTAT_VX_ENSPROB_APCP24H: 2G
  WTIME_RUN_MET_GRIDSTAT_VX_ENSPROB_APCP24H: 01:00:00
  MAXTRIES_RUN_MET_GRIDSTAT_VX_ENSPROB_APCP24H: 2

#----------------------------
# run_met_gridstat_vx_ensprob_refc config parameters
#-----------------------------
task_run_met_gridstat_vx_ensprob_refc:
  TN_RUN_MET_GRIDSTAT_VX_ENSPROB_REFC: "run_MET_GridStat_vx_ensprob_REFC"
  NNODES_RUN_MET_GRIDSTAT_VX_ENSPROB_REFC: 1
  PPN_RUN_MET_GRIDSTAT_VX_ENSPROB_REFC: 1
  MEM_RUN_MET_GRIDSTAT_VX_ENSPROB_REFC: 2G
  WTIME_RUN_MET_GRIDSTAT_VX_ENSPROB_REFC: 01:00:00
  MAXTRIES_RUN_MET_GRIDSTAT_VX_ENSPROB_REFC: 2

#----------------------------
# run_met_gridstat_vx_ensprob_retop config parameters
#-----------------------------
task_run_met_gridstat_vx_ensprob_retop:
  TN_RUN_MET_GRIDSTAT_VX_ENSPROB_RETOP: "run_MET_GridStat_vx_ensprob_RETOP"
  NNODES_RUN_MET_GRIDSTAT_VX_ENSPROB_RETOP: 1
  PPN_RUN_MET_GRIDSTAT_VX_ENSPROB_RETOP: 1
  MEM_RUN_MET_GRIDSTAT_VX_ENSPROB_RETOP: 2G
  WTIME_RUN_MET_GRIDSTAT_VX_ENSPROB_RETOP: 01:00:00
  MAXTRIES_RUN_MET_GRIDSTAT_VX_ENSPROB_RETOP: 2

#----------------------------
# run_met_pointstat_vx_ensprob_sfc config parameters
#-----------------------------
task_run_met_pointstat_vx_ensprob_sfc:
  TN_RUN_MET_POINTSTAT_VX_ENSPROB_SFC: "run_MET_PointStat_vx_ensprob_SFC"
  NNODES_RUN_MET_POINTSTAT_VX_ENSPROB_SFC: 1
  PPN_RUN_MET_POINTSTAT_VX_ENSPROB_SFC: 1
  MEM_RUN_MET_POINTSTAT_VX_ENSPROB_SFC: 2G
  WTIME_RUN_MET_POINTSTAT_VX_ENSPROB_SFC: 01:00:00
  MAXTRIES_RUN_MET_POINTSTAT_VX_ENSPROB_SFC: 2

#----------------------------
# run_met_pointstat_vx_ensprob_upa config parameters
#-----------------------------
task_run_met_pointstat_vx_ensprob_upa:
  TN_RUN_MET_POINTSTAT_VX_ENSPROB_UPA: "run_MET_PointStat_vx_ensprob_UPA"
  NNODES_RUN_MET_POINTSTAT_VX_ENSPROB_UPA: 1
  PPN_RUN_MET_POINTSTAT_VX_ENSPROB_UPA: 1
  MEM_RUN_MET_POINTSTAT_VX_ENSPROB_UPA: 2G
  WTIME_RUN_MET_POINTSTAT_VX_ENSPROB_UPA: 01:00:00
  MAXTRIES_RUN_MET_POINTSTAT_VX_ENSPROB_UPA: 2

#----------------------------
# AQM_ICS config parameters
#-----------------------------
task_aqm_ics:
  TN_AQM_ICS: "aqm_ics"
  NNODES_AQM_ICS: 1
  PPN_AQM_ICS: 1
  WTIME_AQM_ICS: 00:30:00
  MAXTRIES_AQM_ICS: 2

#----------------------------
# AQM_LBCS config parameters
#-----------------------------
task_aqm_lbcs:
  TN_AQM_LBCS: "aqm_lbcs"
  NNODES_AQM_LBCS: 1
  PPN_AQM_LBCS: 24
  WTIME_AQM_LBCS: 00:30:00
  MAXTRIES_AQM_LBCS: 2

#----------------------------
# NEXUS_GFS_SFC config parameters
#-----------------------------
task_nexus_gfs_sfc:
  TN_NEXUS_GFS_SFC: "nexus_gfs_sfc"
  NNODES_NEXUS_GFS_SFC: 1
  PPN_NEXUS_GFS_SFC: 1
  MEM_NEXUS_GFS_SFC: 2G
  WTIME_NEXUS_GFS_SFC: 00:30:00
  MAXTRIES_NEXUS_GFS_SFC: 2
  NEXUS_GFS_SFC_OFFSET_HRS: 0

#----------------------------
# NEXUS_EMISSION config parameters
#-----------------------------
task_nexus_emission:
  TN_NEXUS_EMISSION: "nexus_emission"
  NNODES_NEXUS_EMISSION: 4
  PPN_NEXUS_EMISSION: '{{ platform.NCORES_PER_NODE // OMP_NUM_THREADS_NEXUS_EMISSION }}'
  WTIME_NEXUS_EMISSION: 01:00:00
  MAXTRIES_NEXUS_EMISSION: 2
  KMP_AFFINITY_NEXUS_EMISSION: "scatter"
  OMP_NUM_THREADS_NEXUS_EMISSION: 2
  OMP_STACKSIZE_NEXUS_EMISSION: "1024m"

#----------------------------
# NEXUS_POST_SPLIT config parameters
#-----------------------------
task_nexus_post_split:
  TN_NEXUS_POST_SPLIT: "nexus_post_split"
  NNODES_NEXUS_POST_SPLIT: 1
  PPN_NEXUS_POST_SPLIT: 1
  WTIME_NEXUS_POST_SPLIT: 00:30:00
  MAXTRIES_NEXUS_POST_SPLIT: 2

#----------------------------
# FIRE_EMISSION config parameters
#-----------------------------
task_fire_emission:
  TN_FIRE_EMISSION: "fire_emission"
  NNODES_FIRE_EMISSION: 1
  PPN_FIRE_EMISSION: 1
  MEM_FIRE_EMISSION: 2G
  WTIME_FIRE_EMISSION: 00:30:00
  MAXTRIES_FIRE_EMISSION: 2
  AQM_FIRE_FILE_OFFSET_HRS: 0

#----------------------------
# POINT_SOURCE config parameters
#-----------------------------
task_point_source:
  TN_POINT_SOURCE: "point_source"
  NNODES_POINT_SOURCE: 1
  PPN_POINT_SOURCE: 1 
  WTIME_POINT_SOURCE: 01:00:00
  MAXTRIES_POINT_SOURCE: 2

#----------------------------
# PRE_POST_STAT config parameters
#-----------------------------
task_pre_post_stat:
  TN_PRE_POST_STAT: "pre_post_stat"
  NNODES_PRE_POST_STAT: 1
  PPN_PRE_POST_STAT: 1
  WTIME_PRE_POST_STAT: 00:30:00
  MAXTRIES_PRE_POST_STAT: 2

#----------------------------
# POST_STAT_O3 config parameters
#-----------------------------
task_post_stat_o3:
  TN_POST_STAT_O3: "post_stat_o3"
  NNODES_POST_STAT_O3: 1
  PPN_POST_STAT_O3: 1
  MEM_POST_STAT_O3: 120G 
  WTIME_POST_STAT_O3: 00:30:00
  MAXTRIES_POST_STAT_O3: 2
  KMP_AFFINITY_POST_STAT_O3: "scatter"
  OMP_NUM_THREADS_POST_STAT_O3: 1
  OMP_STACKSIZE_POST_STAT_O3: "2056M"

#----------------------------
# POST_STAT_PM25 config parameters
#-----------------------------
task_post_stat_pm25:
  TN_POST_STAT_PM25: "post_stat_pm25"
  NNODES_POST_STAT_PM25: 1
  PPN_POST_STAT_PM25: 1
  MEM_POST_STAT_PM25: 120G
  WTIME_POST_STAT_PM25: 00:30:00
  MAXTRIES_POST_STAT_PM25: 2
  KMP_AFFINITY_POST_STAT_PM25: "scatter"
  OMP_NUM_THREADS_POST_STAT_PM25: 1
  OMP_STACKSIZE_POST_STAT_PM25: "2056M"

#----------------------------
# BIAS_CORRECTION_O3 config parameters
#-----------------------------
task_bias_correction_o3:
  TN_BIAS_CORRECTION_O3: "bias_correction_o3"
  NNODES_BIAS_CORRECTION_O3: 1
  PPN_BIAS_CORRECTION_O3: 1
  MEM_BIAS_CORRECTION_O3: 120G
  WTIME_BIAS_CORRECTION_O3: 00:30:00
  MAXTRIES_BIAS_CORRECTION_O3: 2
  KMP_AFFINITY_BIAS_CORRECTION_O3: "scatter"
  OMP_NUM_THREADS_BIAS_CORRECTION_O3: 32
  OMP_STACKSIZE_BIAS_CORRECTION_O3: "2056M"

#----------------------------
# BIAS_CORRECTION_PM25 config parameters
#-----------------------------
task_bias_correction_pm25:
  TN_BIAS_CORRECTION_PM25: "bias_correction_pm25"
  NNODES_BIAS_CORRECTION_PM25: 1
  PPN_BIAS_CORRECTION_PM25: 1
  MEM_BIAS_CORRECTION_PM25: 120G
  WTIME_BIAS_CORRECTION_PM25: 00:30:00
  MAXTRIES_BIAS_CORRECTION_PM25: 2
  KMP_AFFINITY_BIAS_CORRECTION_PM25: "scatter"
  OMP_NUM_THREADS_BIAS_CORRECTION_PM25: 32
  OMP_STACKSIZE_BIAS_CORRECTION_PM25: "2056M"

#----------------------------
# global config parameters
#-----------------------------
global:
  #
  #-----------------------------------------------------------------------
  #
  # Set parameters associated with outputting satellite fields in the UPP
  # grib2 files using the Community Radiative Transfer Model (CRTM).
  #
  # USE_CRTM:
  # Flag that defines whether external CRTM coefficient files have been
  # staged by the user in order to output synthetic satellite products
  # available within the UPP. If this is set to true, then the workflow
  # will check for these files in the directory CRTM_DIR. Otherwise, it is
  # assumed that no satellite fields are being requested in the UPP
  # configuration.
  #
  # CRTM_DIR:
  # This is the path to the top CRTM fix file directory. This is only used
  # if USE_CRTM is set to true.
  #
  #-----------------------------------------------------------------------
  #
  USE_CRTM: false
  CRTM_DIR: ""
  #
  #-----------------------------------------------------------------------
  #
  # Set parameters associated with running ensembles.  Definitions:
  #
  # DO_ENSEMBLE:
  # Flag that determines whether to run a set of ensemble forecasts (for
  # each set of specified cycles).  If this is set to true, NUM_ENS_MEMBERS
  # forecasts are run for each cycle, each with a different set of stochastic
  # seed values.  Otherwise, a single forecast is run for each cycle.
  #
  # NUM_ENS_MEMBERS:
  # The number of ensemble members to run if DO_ENSEMBLE is set to true.
  # This variable also controls the naming of the ensemble member directories.
  # For example, if this is set to 8, the member directories will be named 
  # mem1, mem2, ..., mem8. Not used if DO_ENSEMBLE is set to false.
  #
  # ENSMEM_NAMES:
  # A list of names for the ensemble member names following the format
  # mem001, mem002, etc.
  #
  # FV3_NML_ENSMEM_FPS:
  # Paths to the ensemble member corresponding namelists in the
  # experiment directory
  #
  # ENS_TIME_LAG_HRS:
  # Time lag (in hours) to use for each ensemble member.
  #
  #-----------------------------------------------------------------------
  #
  DO_ENSEMBLE: false
  NUM_ENS_MEMBERS: 0
  ENSMEM_NAMES: '{% for m in range(NUM_ENS_MEMBERS) %} "mem%03d, " % m {% endfor %}'
  FV3_NML_ENSMEM_FPS: '{% for mem in ENSMEM_NAMES %}{{ [EXPTDIR, "%s_%s" % FV3_NML_FN, mem]|path_join }}{% endfor %}'
  ENS_TIME_LAG_HRS: '[ {% for m in range(NUM_ENS_MEMBERS) %} 0, {% endfor %} ]'
  #
  #-----------------------------------------------------------------------
  #
  # Set default ad-hoc stochastic physics options.
  # For detailed documentation of these parameters, see:
  # https://stochastic-physics.readthedocs.io/en/ufs_public_release/namelist_options.html
  #
  #-----------------------------------------------------------------------
  #
  DO_SHUM: false
  DO_SPPT: false
  DO_SKEB: false
  ISEED_SPPT: 1
  ISEED_SHUM: 2
  ISEED_SKEB: 3
  NEW_LSCALE: true
  SHUM_MAG: 0.006 #Variable "shum" in input.nml
  SHUM_LSCALE: 150000
  SHUM_TSCALE: 21600 #Variable "shum_tau" in input.nml
  SHUM_INT: 3600 #Variable "shumint" in input.nml
  SPPT_MAG: 0.7 #Variable "sppt" in input.nml
  SPPT_LOGIT: true
  SPPT_LSCALE: 150000
  SPPT_TSCALE: 21600 #Variable "sppt_tau" in input.nml
  SPPT_INT: 3600 #Variable "spptint" in input.nml
  SPPT_SFCLIMIT: true
  SKEB_MAG: 0.5 #Variable "skeb" in input.nml
  SKEB_LSCALE: 150000
  SKEB_TSCALE: 21600 #Variable "skeb_tau" in input.nml
  SKEB_INT: 3600 #Variable "skebint" in input.nml
  SKEBNORM: 1
  SKEB_VDOF: 10
  USE_ZMTNBLCK: false
  #
  #-----------------------------------------------------------------------
  #
  # Set default SPP stochastic physics options. Each SPP option is an array, 
  # applicable (in order) to the scheme/parameter listed in SPP_VAR_LIST. 
  # Enter each value of the array in config.yaml as shown below without commas
  # or single quotes (e.g., SPP_VAR_LIST=( "pbl" "sfc" "mp" "rad" "gwd" ). 
  # Both commas and single quotes will be added by Jinja when creating the
  # namelist.
  #
  # Note that SPP is currently only available for specific physics schemes 
  # used in the RAP/HRRR physics suite.  Users need to be aware of which SDF
  # is chosen when turning this option on. 
  #
  # Patterns evolve and are applied at each time step.
  #
  #-----------------------------------------------------------------------
  #
  DO_SPP: false
  SPP_VAR_LIST: [ "pbl", "sfc", "mp", "rad", "gwd" ]
  SPP_MAG_LIST: [ 0.2, 0.2, 0.75, 0.2, 0.2 ] #Variable "spp_prt_list" in input.nml
  SPP_LSCALE: [ 150000.0, 150000.0, 150000.0, 150000.0, 150000.0 ]
  SPP_TSCALE: [ 21600.0, 21600.0, 21600.0, 21600.0, 21600.0 ] #Variable "spp_tau" in input.nml
  SPP_SIGTOP1: [ 0.1, 0.1, 0.1, 0.1, 0.1 ]
  SPP_SIGTOP2: [ 0.025, 0.025, 0.025, 0.025, 0.025 ]
  SPP_STDDEV_CUTOFF: [ 1.5, 1.5, 2.5, 1.5, 1.5 ]
  ISEED_SPP: [ 4, 5, 6, 7, 8 ]
  #
  #-----------------------------------------------------------------------
  #
  # Turn on SPP in Noah or RUC LSM (support for Noah MP is in progress).
  # Please be aware of the SDF that you choose if you wish to turn on LSM
  # SPP.
  #
  # SPP in LSM schemes is handled in the &nam_sfcperts namelist block 
  # instead of in &nam_sppperts, where all other SPP is implemented.
  #
  # Perturbations to soil moisture content (SMC) are only applied at the 
  # first time step.
  #
  # LSM perturbations include SMC - soil moisture content (volume fraction),
  # VGF - vegetation fraction, ALB - albedo, SAL - salinity, 
  # EMI - emissivity, ZOL - surface roughness (cm), and STC - soil temperature.
  #
  # Only five perturbations at a time can be applied currently, but all seven
  # are shown below.  In addition, only one unique iseed value is allowed 
  # at the moment, and is used for each pattern.
  #
  DO_LSM_SPP: false #If true, sets lndp_type=2
  LSM_SPP_TSCALE: [ 21600, 21600, 21600, 21600, 21600, 21600, 21600 ]
  LSM_SPP_LSCALE: [ 150000, 150000, 150000, 150000, 150000, 150000, 150000 ]
  ISEED_LSM_SPP: [ 9 ]
  LSM_SPP_VAR_LIST: [ "smc", "vgf", "alb", "sal", "emi", "zol", "stc" ]
  LSM_SPP_MAG_LIST: [ 0.017, 0.001, 0.001, 0.001, 0.001, 0.001, 0.2 ]
  #
  #-----------------------------------------------------------------------
  # 
  # HALO_BLEND:
  # Number of rows into the computational domain that should be blended 
  # with the LBCs.  To shut halo blending off, this can be set to zero.
  #
  #-----------------------------------------------------------------------
  #
  HALO_BLEND: 10
  #
  #-----------------------------------------------------------------------
  #

#----------------------------
# verification (vx) parameters
#-----------------------------
verification:
  # Move some of the following to another section at some point.
  #
  # GET_OBS_LOCAL_MODULE_FN:
  # Local task modulefile name for all GET_OBS_* tasks.
  #
  GET_OBS_LOCAL_MODULE_FN: 'get_obs'
  #
  # Templates for CCPA observation files.
  #
  OBS_CCPA_APCP01h_FN_TEMPLATE: '{valid?fmt=%Y%m%d}/ccpa.t{valid?fmt=%H}z.01h.hrap.conus.gb2'
  OBS_CCPA_APCPgt01h_FN_TEMPLATE: '${OBS_CCPA_APCP01h_FN_TEMPLATE}_a${ACCUM_HH}h.nc'
  OBS_NDAS_SFCorUPA_FN_TEMPLATE: 'prepbufr.ndas.{valid?fmt=%Y%m%d%H}'
  OBS_NDAS_SFCorUPA_FN_METPROC_TEMPLATE: '${OBS_NDAS_SFCorUPA_FN_TEMPLATE}.nc'
  #
  # VX_LOCAL_MODULE_FN:
  # Name (without extension) of the local module file for running the vx
  # tasks in the workflow.
  #
  VX_LOCAL_MODULE_FN: 'run_vx'
  #
  # RUN_TASKS_METVX_DET:
  # Flag that specifies whether to run deterministic verification.  If set
  # to True, this will run deterministic vx on the post-processed forecast
  # output.  This post-processed output may consist of a single forecast
  # or an ensemble of foreasts, and it may be staged from previous runs of
  # the SRW App or may be generated by running the TN_RUN_FCST task as part
  # of the current SRW-App-generated experiment.
  #
  # RUN_TASKS_METVX_ENS:
  # Flag that specifies whether to run ensemble verification.  The ensemble
  # forecast output on which vx will be run may be staged or generated by
  # running an ensemble of forecasts with the weather model as part of the
  # current SRW-App-generated experiment.
  #
  RUN_TASKS_METVX_DET: False
  RUN_TASKS_METVX_ENS: False
  #
  # VX_FCST_MODEL_NAME:
  # String that specifies a descriptive name for the model being verified.
  # This is used in forming the names of the verification output files as
  # well as in the contents of those files.
  #
  # VX_FIELDS:
  # The fields or groups of fields on which to run verification.
  #
  # VX_APCP_ACCUMS_HH:
  # The 2-digit accumulation periods (in units of hours) to consider for
  # APCP (accumulated precipitation).  If VX_FIELDS contains "APCP", then
  # VX_APCP_ACCUMS_HH must contain at least one element.  If not,
  # VX_APCP_ACCUMS_HH will be ignored.
  #
  VX_FCST_MODEL_NAME: '{{ nco.NET }}.{{ task_run_post.POST_OUTPUT_DOMAIN_NAME }}'
  VX_FIELDS: [ "APCP", "REFC", "RETOP", "SFC", "UPA" ]
  VX_APCP_ACCUMS_HH: [ "01", "03", "06", "24" ]
  #
  # VX_FCST_INPUT_BASEDIR:
  # Location of top-level directory containing forecast (but not obs) files
  # that will be used as input into METplus for verification.  If not
  # specified, this gets set to EXPTDIR.
  #
  # VX_OUTPUT_BASEDIR:
  # Top-level directory in which METplus will place its output.
  #
  VX_FCST_INPUT_BASEDIR: '{{ workflow.EXPTDIR if ((workflow_switches.RUN_TASK_RUN_FCST and task_run_fcst.WRITE_DOPOST) or workflow_switches.RUN_TASK_RUN_POST) }}'
  VX_OUTPUT_BASEDIR: '{{ workflow.EXPTDIR }}'
  #
  # File name and path templates are used in the verification tasks.
  #
  FCST_SUBDIR_TEMPLATE: '{init?fmt=%Y%m%d%H?shift=-${time_lag}}${SLASH_ENSMEM_SUBDIR_OR_NULL}/postprd'
  FCST_FN_TEMPLATE: '${NET}.t{init?fmt=%H?shift=-${time_lag}}z.prslev.f{lead?fmt=%HHH?shift=${time_lag}}.${POST_OUTPUT_DOMAIN_NAME}.grib2'
  FCST_FN_METPROC_TEMPLATE: '${NET}.t{init?fmt=%H}z.prslev.f{lead?fmt=%HHH}.${POST_OUTPUT_DOMAIN_NAME}_a${ACCUM_HH}h.nc'
  #
  # For verification tasks that need observational data, this specifies
  # the maximum number of observation files that may be missing.  If more
  # than this number are missing, the verification task will error out.
  #
  # Note that this is a crude way of checking that there are enough obs to
  # conduct verification since this number should probably depend on the
  # field being verified, the time interval between observations, the
  # length of the forecast, etc.  An alternative may be to specify the
  # maximum allowed fraction of obs files that can be missing (i.e. the
  # number missing divided by the number that are expected to exist).
  #
  NUM_MISSING_OBS_FILES_MAX: 2

#----------------------------
# CPL_AQM config parameters
#-----------------------------
cpl_aqm_parm:
  #
  #-----------------------------------------------------------------------
  #
  # CPL_AQM:
  # Coupling flag for air quality modeling
  #
  # DO_AQM_DUST:
  # Flag turning on/off AQM dust option in AQM_RC
  #
  # DO_AQM_CANOPY
  # Flag turning on/off AQM canopy option in AQM_RC
  # 
  # DO_AQM_PRODUCT
  # Flag turning on/off AQM output products in AQM_RC
  # 
  # DO_AQM_CHEM_LBCS:
  # Add chemical LBCs to chemical LBCs
  # 
  # DO_AQM_GEFS_LBCS:
  # Add GEFS aerosol LBCs to chemical LBCs
  #
  # DO_AQM_SAVE_AIRNOW_HIST:
  # Save bias-correction airnow training data
  #
  # DO_AQM_SAVE_FIRE:
  # Archive fire emission file to HPSS
  #
  # AQM_CONFIG_DIR:
  # Configuration directory for AQM
  # 
  # DCOMINbio:
  # Path to the directory containing AQM bio files
  # 
  # AQM_BIO_FILE:
  # File name of AQM BIO file
  #
  # DCOMINdust:
  # Path to the directory containing AQM dust file
  #
  # AQM_DUST_FILE_PREFIX:
  # Frefix of AQM dust file
  #
  # AQM_DUST_FILE_SUFFIX:
  # Suffix and extension of AQM dust file
  #
  # DCOMINcanopy:
  # Path to the directory containing AQM canopy files
  # 
  # AQM_CANOPY_FILE_PREFIX:
  # File name of AQM canopy file
  #
  # AQM_CANOPY_FILE_SUFFIX:
  # Suffix and extension of AQM CANOPY file
  # 
  # DCOMINfire:
  # Path to the directory containing AQM fire emission files
  # 
  # AQM_FIRE_FILE_PREFIX:
  # Prefix of AQM FIRE file
  # 
  # AQM_FIRE_FILE_SUFFIX:
  # Suffix and extension of AQM FIRE file
  #
  # AQM_FIRE_ARCHV_DIR:
  # Path to the archive directory for RAVE emission files on HPSS
  #
  # AQM_RC_FIRE_FREQUENCY:
  # Fire frequency in aqm.rc
  #
  # AQM_RC_PRODUCT_FN:
  # File name of AQM output products
  #
  # AQM_RC_PRODUCT_FREQUENCY:
  # Frequency of AQM output products
  #
  # DCOMINchem_lbc:
  # Path to the directory containing chemical LBC files
  # 
  # AQM_LBCS_FILES:
  # File name of chemical LBCs
  #
  # DCOMINgefs:
  # Path to the directory containing GEFS aerosol LBC files
  #
  # AQM_GEFS_FILE_PREFIX:
  # Prefix of AQM GEFS file ("geaer" or "gfs")
  #
  # AQM_GEFS_FILE_CYC:
  # Cycle of the GEFS aerosol LBC files only if it is fixed
  # 
  # NEXUS_INPUT_DIR:
  # Same as GRID_DIR but for the the air quality emission generation task.
  # Should be blank for the default value specified in setup.sh
  # 
  # NEXUS_FIX_DIR:
  # Directory containing grid_spec files as the input file of nexus
  # 
  # NEXUS_GRID_FN:
  # File name of the input grid_spec file of nexus
  #
  # NUM_SPLIT_NEXUS:
  # Number of split nexus emission tasks
  #
  # NEXUS_GFS_SFC_OFFSET_HRS: 0
  # Time offset when retrieving gfs surface data files
  # 
  # NEXUS_GFS_SFC_DIR:
  # Path to directory containing GFS surface data files
  # This is set to COMINgfs when DO_REAL_TIME=TRUE. 
  #
  # NEXUS_GFS_SFC_ARCHV_DIR: 
  # Path to archive directory for gfs surface files on HPSS
  #
  # DCOMINpt_src:
  # Path to the directory containing point source files
  #
  # DCOMINairnow:
  # Path to the directory containing AIRNOW observation data
  #
  # COMINbicor:
  # Path of reading in historical training data for biascorrection 
  #
  # COMOUTbicor:
  # Path to save the current cycle's model output and AirNow obs as training data for future use
  # $COMINbicor and $COMOUTbicor can be distuigshed by the ${yyyy}${mm}$dd under the same location
  #-----------------------------------------------------------------------
  #
  CPL_AQM: false

  DO_AQM_DUST: true
  DO_AQM_CANOPY: false
  DO_AQM_PRODUCT: true
  DO_AQM_CHEM_LBCS: true
  DO_AQM_GEFS_LBCS: false
  DO_AQM_SAVE_AIRNOW_HIST: false
  DO_AQM_SAVE_FIRE: false

  AQM_CONFIG_DIR: ""
  DCOMINbio_dfv: ""
  AQM_BIO_FILE: "BEIS_SARC401.ncf"

  DCOMINdust_dfv: "/path/to/dust/dir"
  AQM_DUST_FILE_PREFIX: "FENGSHA_p8_10km_inputs"
  AQM_DUST_FILE_SUFFIX: ".nc"

  DCOMINcanopy_dfv: "/path/to/canopy/dir"
  AQM_CANOPY_FILE_PREFIX: "gfs.t12z.geo"
  AQM_CANOPY_FILE_SUFFIX: ".canopy_regrid.nc"

  DCOMINfire_dfv: ""
  AQM_FIRE_FILE_PREFIX: "GBBEPx_C401GRID.emissions_v003"
  AQM_FIRE_FILE_SUFFIX: ".nc"
  AQM_FIRE_ARCHV_DIR: "/path/to/archive/dir/for/RAVE/on/HPSS"

  AQM_RC_FIRE_FREQUENCY: "static"
  AQM_RC_PRODUCT_FN: "aqm.prod.nc"
  AQM_RC_PRODUCT_FREQUENCY: "hourly"

  DCOMINchem_lbcs_dfv: ""
  AQM_LBCS_FILES: "am4_bndy_c793.2019<MM>.v1.nc"

  DCOMINgefs_dfv: ""
  AQM_GEFS_FILE_PREFIX: "geaer"
  AQM_GEFS_FILE_CYC: ""

  NEXUS_INPUT_DIR: ""
  NEXUS_FIX_DIR: ""
  NEXUS_GRID_FN: "grid_spec_GSD_HRRR_25km.nc"
  NUM_SPLIT_NEXUS: 3
  NEXUS_GFS_SFC_DIR: ""
  NEXUS_GFS_SFC_ARCHV_DIR: "/NCEPPROD/hpssprod/runhistory"

  DCOMINpt_src_dfv: "/path/to/point/source/base/directory"

  DCOMINairnow_dfv: "/path/to/airnow/obaservation/data"
  COMINbicor: "/path/to/historical/airnow/data/dir"
  COMOUTbicor: "/path/to/historical/airnow/data/dir"
